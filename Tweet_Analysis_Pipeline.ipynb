{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t8Jsbp44oqy"
      },
      "outputs": [],
      "source": [
        "%pip install HanTa\n",
        "%pip install lda\n",
        "%pip install pyldavis\n",
        "%pip install germalemma == 0.1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LJ-OS1i_4oq6",
        "outputId": "e9e8173c-1cf6-486a-b32b-a8da5e811e77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Softwares\\anaconda3\\envs\\dbse-project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Er Pravin\n",
            "[nltk_data]     Pandey\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Er Pravin\n",
            "[nltk_data]     Pandey\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Er Pravin\n",
            "[nltk_data]     Pandey\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Er Pravin\n",
            "[nltk_data]     Pandey\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# import lda\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from HanTa import HanoverTagger as ht\n",
        "from nltk.corpus import stopwords\n",
        "from germalemma import GermaLemma\n",
        "from bertopic import BERTopic\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis\n",
        "from bertopic import BERTopic\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "en_tagger = ht.HanoverTagger(\"morphmodel_en.pgz\")\n",
        "de_tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eWFsCxgf4oq9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:80: DeprecationWarning: invalid escape sequence \\d\n",
            "<>:80: DeprecationWarning: invalid escape sequence \\d\n",
            "C:\\Users\\Er Pravin Pandey\\AppData\\Local\\Temp\\ipykernel_13168\\379271308.py:80: DeprecationWarning: invalid escape sequence \\d\n",
            "  read_df[\"text\"] = read_df[\"text\"].str.replace(\"\\d+\", \"\")  # removes number\n"
          ]
        }
      ],
      "source": [
        "# Text Analysis Functions\n",
        "\n",
        "\n",
        "def clean_data(filename: str):\n",
        "    \"\"\"\n",
        "    Method to preprocess tweet data by removing emojis, mentions, tags and stopwords.\n",
        "        :params: `filename (str)`: Data file name to be processed.\n",
        "        :return: \n",
        "                 `en_df`: tweets dataframe of english language\n",
        "                 `de_df`: tweets dataframe of german language\n",
        "                 `read_df`: tweets dataframe of all languages (Entire Corpus)\n",
        "    \"\"\"\n",
        "\n",
        "    def remove_emojis(data):\n",
        "        \"\"\"\n",
        "        Method to remove emojis from tweets\n",
        "            :params: `data (str)`: text of tweet data.\n",
        "            :return: `data (str)`: text of tweet data without emojis.\n",
        "        \"\"\"\n",
        "        emoj = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U000024C2-\\U0001F251\"\n",
        "            \"\\U0001f926-\\U0001f937\"\n",
        "            \"\\U00010000-\\U0010ffff\"\n",
        "            \"\\u2640-\\u2642\"\n",
        "            \"\\u2600-\\u2B55\"\n",
        "            \"\\u200d\"\n",
        "            \"\\u23cf\"\n",
        "            \"\\u23e9\"\n",
        "            \"\\u231a\"\n",
        "            \"\\ufe0f\"  # dingbats\n",
        "            \"\\u3030\"\n",
        "            \"]+\",\n",
        "            re.UNICODE,\n",
        "        )\n",
        "        return re.sub(emoj, \"\", data)\n",
        "\n",
        "    # Removing mentions and hashtags and url\n",
        "    def remove_mentions_and_tags(text):\n",
        "        \"\"\"\n",
        "        Method to remove mentions and tags from tweets\n",
        "            :params: `text (str)`: text of tweet data.\n",
        "            :return: `text (str)`: text tweet data without mentions and tags.\n",
        "        \"\"\"\n",
        "        text = re.sub(r\"@\\S*\", \"\", text)\n",
        "        text = re.sub(r\"http\\S+\", \"\", text)\n",
        "        return re.sub(r\"#\\S*\", \"\", text)\n",
        "\n",
        "    def remove_stopwords(df):\n",
        "        \"\"\"\n",
        "        Method to remove mentions and tags from tweets\n",
        "            :params: `df (Dataframe)`: Dataframe object of tweet data.\n",
        "            :return: `df (Dataframe)`: Dataframe object with new column `tweet_without_stopwords`\n",
        "                      containing tweet data without stopwords.\n",
        "        \"\"\"\n",
        "        stop_en = stopwords.words(\"english\")\n",
        "        stop_de = stopwords.words(\"german\")\n",
        "        df[\"tweet_without_stopwords\"] = df[\"text\"].apply(\n",
        "            lambda x: \" \".join([word for word in x.split() if word not in (stop_de)]))\n",
        "        df[\"tweet_without_stopwords\"] = df[\"tweet_without_stopwords\"].apply(\n",
        "            lambda x: \" \".join(\n",
        "                [word for word in x.split() if word not in (stop_en)])\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    read_df = pd.read_parquet(filename)\n",
        "    read_df = read_df.loc[read_df[\"tweet_type\"] != \"retweet\"]\n",
        "    read_df = read_df.drop_duplicates(subset=[\"tweet_id\"], ignore_index=True)\n",
        "    read_df.text = read_df.text.apply(remove_emojis)\n",
        "    read_df.text = read_df.text.apply(remove_mentions_and_tags)\n",
        "    read_df[\"text\"] = read_df[\"text\"].str.replace(\n",
        "        r\"[^\\w\\s]+\", \"\")  # remove punctuations\n",
        "    read_df[\"text\"] = read_df[\"text\"].str.replace(\"\\d+\", \"\")  # removes number\n",
        "    read_df = remove_stopwords(read_df)\n",
        "    en_df = read_df[read_df.tweet_language == \"en\"]\n",
        "    de_df = read_df[read_df.tweet_language == \"de\"]\n",
        "    return en_df, de_df, read_df\n",
        "\n",
        "\n",
        "def extract_adj_noun(df, tagger: ht.HanoverTagger):\n",
        "    \"\"\"\n",
        "    Method to remove mentions and tags from tweets\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data.\n",
        "                 `tagger (HanoverTagger)`: HanTa Tagger object to extraxt adjectives and noun from text.\n",
        "        :return: `df (Dataframe)`: Dataframe object with new columns `adj` and `noun` containing extracted adjectives and nouns.\n",
        "    \"\"\"\n",
        "\n",
        "    def noun(text, tagger):\n",
        "        \"\"\"\n",
        "        Method to extract nouns\n",
        "        :params: \n",
        "                 `text (str)`: text of tweet data.\n",
        "                 `tagger (HanoverTagger)`: HanTa Tagger object to extraxt adjectives and noun from text.\n",
        "        :return: `tokens (array)`: extracted nouns.\n",
        "        \"\"\"\n",
        "        words = nltk.word_tokenize(text)\n",
        "        tokens = [word for (word, x, pos) in tagger.tag_sent(\n",
        "            words, taglevel=1) if pos == \"NN\"]\n",
        "        return tokens\n",
        "\n",
        "    def adj(text, tagger):\n",
        "        \"\"\"\n",
        "        Method to extract adjectives\n",
        "        :params: \n",
        "                 `text (str)`: text of tweet data.\n",
        "                 `tagger (HanoverTagger)`: HanTa Tagger object to extraxt adjectives and noun from text.\n",
        "        :return: `tokens (array)`: extracted adjectives.\n",
        "        \"\"\"\n",
        "        words = nltk.word_tokenize(text)\n",
        "        tokens = [word for (word, x, pos) in tagger.tag_sent(\n",
        "            words, taglevel=1) if pos == \"ADJ\"]\n",
        "        return tokens\n",
        "\n",
        "    def to_lowercase(text):\n",
        "        \"\"\"\n",
        "        Method to chnage text case to lowercase\n",
        "        :params: \n",
        "                 `text (str)`: text of tweet data.\n",
        "        :return: `tokens (array)`: lower case tokens.\n",
        "        \"\"\"\n",
        "        token = []\n",
        "        for i in range(len(text)):\n",
        "            val = text[i].lower()\n",
        "            token.append(val)\n",
        "        return token\n",
        "    \n",
        "    def word_token(x):\n",
        "        return ' '.join([w for w in x])\n",
        "\n",
        "    df[\"noun\"] = df.tweet_without_stopwords.apply(noun, tagger=tagger)\n",
        "    df[\"adj\"] = df.tweet_without_stopwords.apply(adj, tagger=tagger)\n",
        "    df.noun = df.noun.apply(to_lowercase)\n",
        "    df.noun = df.noun.apply(word_token)\n",
        "    df.adj = df.adj.apply(word_token)\n",
        "    return df\n",
        "\n",
        "\n",
        "def lementize_en_text(df):\n",
        "    \"\"\"\n",
        "    Method to Lementize English Text\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data.\n",
        "        :return: \n",
        "                 `df (Dataframe)`: Dataframe object with updated `adj` and `noun` containing lemantized adjectives and nouns.\n",
        "                 `dict_docs (Dict)`: Dictionary of lemantized noun from text.\n",
        "    \"\"\"\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    def lemmatize_text(text):\n",
        "        return [lemmatizer.lemmatize(w, \"n\") for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "    def lemmatize_text_adj(text):\n",
        "        return [lemmatizer.lemmatize(w, \"a\") for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "    df[\"lemma_noun\"] = df.noun.apply(lemmatize_text)\n",
        "    df[\"lemma_adj\"] = df.adj.apply(lemmatize_text_adj)\n",
        "    dict_docs = df.lemma_noun.to_dict()\n",
        "    return df, dict_docs\n",
        "\n",
        "\n",
        "def lementize_de_text(df):\n",
        "    \"\"\"\n",
        "    Method to Lementize German Text\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data.\n",
        "        :return: \n",
        "                 `df (Dataframe)`: Dataframe object with updated `adj` and `noun` containing lemantized adjectives and nouns.\n",
        "                 `dict_docs (Dict)`: Dictionary of lemantized noun from text.\n",
        "    \"\"\"\n",
        "    lemmatizer = GermaLemma()\n",
        "\n",
        "    def lemmatize_noun(x):\n",
        "        arr = []\n",
        "        token = x.split()\n",
        "        for i in range(len(token)):\n",
        "            lemma_noun = lemmatizer.find_lemma(token[i], \"N\")\n",
        "            arr.append(lemma_noun)\n",
        "        return arr\n",
        "\n",
        "    def lemmatize_adj(x):\n",
        "        arr = []\n",
        "        token = x.split()\n",
        "        for i in range(len(token)):\n",
        "            lemma_noun = lemmatizer.find_lemma(token[i], \"ADJ\")\n",
        "            arr.append(lemma_noun)\n",
        "        return arr\n",
        "\n",
        "    df[\"lemma_noun\"] = df.noun.apply(lemmatize_noun)\n",
        "    df[\"lemma_adj\"] = df.adj.apply(lemmatize_adj)\n",
        "    dict_docs = df.lemma_noun.to_dict()\n",
        "    return df, dict_docs\n",
        "\n",
        "\n",
        "def gen_doc_matrix(df):\n",
        "    \"\"\"\n",
        "    Method to generate Document Matrix and ID to Word Matrix.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data.\n",
        "        :return: \n",
        "                 `corpus (Dict)`: Document token Matrix.\n",
        "                 `id2word (Dict)`: ID represenatation of words Dictionary.\n",
        "    \"\"\"\n",
        "    def generate_tokens(tweet):\n",
        "        \"\"\"\n",
        "        Method to generate word array (tokens)\n",
        "        :params: \n",
        "                 `tweet (str)`: tweet text.\n",
        "        :return: \n",
        "                 `words (array)`: words array.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        for i in range(len(tweet)):\n",
        "            word = tweet[i]\n",
        "            # using the if condition because we introduced extra spaces during text cleaning\n",
        "            if word != \"\":\n",
        "                words.append(word)\n",
        "        return words\n",
        "\n",
        "    def create_dictionary(words):\n",
        "        return corpora.Dictionary(words)\n",
        "\n",
        "    def create_document_matrix(tokens, id2word):\n",
        "        \"\"\"\n",
        "        Method to generate document matrix\n",
        "        :params: \n",
        "                 `tokens (array)`: words array.\n",
        "                 `id2word (Dict)`: ID represenatation of words Dictionary.\n",
        "        :return: \n",
        "                 `corpus (Dict)`: document token matrix.\n",
        "        \"\"\"\n",
        "        corpus = []\n",
        "        for text in tokens:\n",
        "            corpus.append(id2word.doc2bow(text))\n",
        "        return corpus\n",
        "\n",
        "    # storing the generated tokens in a new column named 'words'\n",
        "    df[\"tokens\"] = df.lemma_noun.apply(generate_tokens)\n",
        "\n",
        "    # passing the dataframe column having tokens as the argument\n",
        "    id2word = create_dictionary(df.tokens)\n",
        "\n",
        "    # passing the dataframe column having tokens and dictionary\n",
        "    corpus = create_document_matrix(df.tokens, id2word)\n",
        "    return corpus, id2word\n",
        "\n",
        "\n",
        "def fetch_doc_topic(df, corpus, lda_model, ntopics):\n",
        "    \"\"\"\n",
        "    Method to generate Document Topic Matrix.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data.\n",
        "                 `corpus (Dict)`: document token Matrix.\n",
        "                 `lda_model (Gensim LDA Object)`: Lda model object.\n",
        "                 `ntopics (int)`: number of topics\n",
        "        :return: \n",
        "                 `doc_topic (Dict)`: document Topic matrix.\n",
        "                 `doc_topic_df (Dataframe)`: document Topic matrix Dataframe.\n",
        "                 `count_arr (array)`: Count of documents in each topic.\n",
        "    \"\"\"\n",
        "    def get_doc_topic(corpus, model):\n",
        "        \"\"\"\n",
        "        Method to generate document topic matrix\n",
        "        :params: \n",
        "                 `corpus (Dict)`: document token Matrix.\n",
        "                 `model (Gensim LDA Object)`: Lda model object.\n",
        "        :return: \n",
        "                 `doc_topic (array)`: document Topic matrix.\n",
        "        \"\"\"\n",
        "        doc_topic = []\n",
        "        for doc in range(len(corpus)):\n",
        "            try:\n",
        "                doc_topic.append(\n",
        "                    {\n",
        "                        \"Tweet\": df.iloc[doc].text,\n",
        "                        \"Hashtags\": df.iloc[doc].hashtags,\n",
        "                        \"Topic_Probability\": model.__getitem__(corpus[doc], eps=0),\n",
        "                    }\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        return doc_topic\n",
        "\n",
        "    def calc_doc_topic_count(doc_topic, count_arr):\n",
        "        \"\"\"\n",
        "        Method to generate document topic count\n",
        "        :params: \n",
        "                 `doc_topic (Dict)`: document Topic matrix.\n",
        "                 `count_arr (array)`: placeholder array of length ntopics.\n",
        "        :return: \n",
        "                 `count_arr (array)`: Count of documents in each topic.\n",
        "        \"\"\"\n",
        "        for topic in doc_topic:\n",
        "            prob = topic[\"Topic_Probability\"]\n",
        "            big = -99999\n",
        "            for p in prob:\n",
        "                if p[1] > big:\n",
        "                    big = p[1]\n",
        "                    max_top = p[0]\n",
        "            count_arr[max_top] += 1\n",
        "        return count_arr\n",
        "\n",
        "    doc_topic = get_doc_topic(corpus, lda_model)\n",
        "    doc_topic_df = pd.DataFrame(doc_topic)\n",
        "    count_arr = calc_doc_topic_count(doc_topic, [0] * ntopics)\n",
        "    return doc_topic, doc_topic_df, count_arr\n",
        "\n",
        "\n",
        "def doc_topic_pie_chart(ntopics, count_arr):\n",
        "    \"\"\"\n",
        "    Method to generate Pie Chart to display document segregation.\n",
        "        :params: \n",
        "                 `ntopics (int)`: number of topics.\n",
        "                 `count_arr (array)`: Count of documents in each topic.\n",
        "    \"\"\"\n",
        "    topics = []\n",
        "    for i in range(ntopics):\n",
        "        topics.append(i + 1)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(count_arr, labels=topics, autopct=\"%1.1f%%\",\n",
        "            textprops={\"fontsize\": 18})\n",
        "    plt.title(\"Tweets distribution in topics\", fontsize=20)\n",
        "    # plt.legend(data_pie.tweets_count,fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "def fetch_word_topic(lda_model, id2word):\n",
        "    \"\"\"\n",
        "    Method to generate Document Topic Matrix.\n",
        "        :params: \n",
        "                 `lda_model (Gensim LDA Object)`: Lda model object.\n",
        "                 `id2word (Dict)`: ID represenatation of words Dictionary.\n",
        "        :return: \n",
        "                 `word_topic_dict (Dict)`: Word Topic matrix.\n",
        "                 `word_topic_df (Dataframe)`: Word Topic matrix Dataframe.\n",
        "    \"\"\"\n",
        "    def get_topic_to_wordids(model):\n",
        "        \"\"\"\n",
        "        Method to generate word topic probability array.\n",
        "        :params: \n",
        "                 `model (Gensim LDA Object)`: Lda model object.\n",
        "        :return: \n",
        "                 `p (array)`: word topic probability array.\n",
        "        \"\"\"\n",
        "        p = list()\n",
        "        for topicid in range(model.num_topics):\n",
        "            topic = model.state.get_lambda()[topicid]\n",
        "            topic = topic / max(topic)  # normalize to probability dist\n",
        "            p.append(topic)\n",
        "        return p\n",
        "\n",
        "    def create_dict_word_topic(id2word, word_topic):\n",
        "        \"\"\"\n",
        "        Method to generate most probable word topic matrix.\n",
        "        :params: \n",
        "                 `id2word (Dict)`: ID represenatation of words Dictionary.\n",
        "                 `word_topic (array)`: word topic probability array.\n",
        "\n",
        "        :return: \n",
        "                 `word_topic_dict (Dict)`: Most probable word topic matrix.\n",
        "        \"\"\"\n",
        "        word_topic_dict = []\n",
        "        i = 0\n",
        "        for topic in word_topic:\n",
        "            prob_words = []\n",
        "            for word_ind in range(len(topic)):\n",
        "                if topic[word_ind] >= 0.15:\n",
        "                    prob_words.append(id2word[word_ind])\n",
        "            word_topic_dict.append(\n",
        "                {\"Topic\": (i + 1), \"most_prob_words\": prob_words})\n",
        "            i += 1\n",
        "        return word_topic_dict\n",
        "\n",
        "    word_topic = get_topic_to_wordids(lda_model)\n",
        "    word_topic_dict = create_dict_word_topic(id2word, word_topic)\n",
        "    word_topic_df = pd.DataFrame(word_topic_dict)\n",
        "    return word_topic_dict, word_topic_df\n",
        "\n",
        "def print_beauty(df, word_topic_df):\n",
        "    \"\"\"\n",
        "    Custom method to display topic probabilities for certain tweets from df.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: tweet dataset dataframe.\n",
        "                 `word_topic_df (Dataframe)`: Probable word list for each tweet.\n",
        "    \"\"\"\n",
        "    for i in range(6, 10):\n",
        "        print(\"Tweet:\", df.iloc[i].Tweet)\n",
        "        print(\"Hashtags:\", df.iloc[i].Hashtags)\n",
        "        print(\"Topic_Probability:\", df.iloc[i].Topic_Probability)\n",
        "        print(\"----------------------------------------------------------------\\n\")\n",
        "\n",
        "    for i in range(4):\n",
        "        print(\"Topic \", (i + 1))\n",
        "        print(\"Most Probable Word List: \",\n",
        "              word_topic_df.iloc[i].most_prob_words)\n",
        "        print(\"-------------------------------------------------------\\n\")\n",
        "\n",
        "def run_bert_topic_model(df, method: str =\"default\", filename: str = \"BERTopic_model\"):\n",
        "    \"\"\"\n",
        "    Method to run BERTopic model package and fit documents to it.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: tweet dataset dataframe.\n",
        "                 `method (str)`: Sentence Embedding name to be used. `roberta` or `default`\n",
        "        :return: \n",
        "                 `model (BERTopic model)`: Document fitted BERTopic model object.\n",
        "                 `topics (Dict)`: Topic matrix.\n",
        "                 `probabilities (Dict)`: Document topic probabilities matrix.\n",
        "    \"\"\"\n",
        "    if method == \"roberta\":\n",
        "        sen_embed_model = SentenceTransformer(\n",
        "            \"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\")\n",
        "        model = BERTopic(embedding_model=sen_embed_model, nr_topics=\"auto\")\n",
        "    elif method == 'default':\n",
        "        model = BERTopic(\n",
        "            verbose=True, language=\"multilingual\", nr_topics=\"auto\")\n",
        "    docs = df.tweet_without_stopwords.to_list()\n",
        "    topics, probabilities = model.fit_transform(docs)\n",
        "    model.reduce_topics(docs, nr_topics=50)\n",
        "    try:\n",
        "        model.save(filename)\n",
        "    except Exception as e:\n",
        "        print(f\"BERTopic model file selected already exists in storage. Unable to save model.\\n Error: {e}\")\n",
        "    return model, topics, probabilities\n",
        "\n",
        "\n",
        "def load_bert_topic_model(filename: str):\n",
        "    \"\"\"\n",
        "    Method to load document fitted exsiting BERTopic model.\n",
        "        :params: \n",
        "                 `filename (str)`: File name of the BERTopic model.\n",
        "        :return: \n",
        "                 `model (BERTopic model)`: Document fitted BERTopic model object.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = BERTopic.load(filename)\n",
        "    except Exception as e:\n",
        "        print(f\"BERTopic model file not found.\\n Error: {e}\")\n",
        "        model = None\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1YogLZoX4orF"
      },
      "outputs": [],
      "source": [
        "# Hashtag Analysis Functions\n",
        "\n",
        "\n",
        "def extract_most_frequent_hashtags(df):\n",
        "    dates = list()\n",
        "\n",
        "    for row, record in enumerate(df.hashtags):\n",
        "        if record != \"[]\" and df[\"timestamp\"][row][0:7] > \"2021-05\":\n",
        "            dates.append(df[\"timestamp\"][row][0:7])\n",
        "    date = list()\n",
        "    for dt in dates:\n",
        "        if dt not in date:\n",
        "            date.append(dt)\n",
        "\n",
        "    i = 1\n",
        "    j = 6\n",
        "    for i in range(1, 3):\n",
        "        for j in range(1, 13):\n",
        "            if i == 1 and j > 5 and j < 10 or i == 2 and j < 10:\n",
        "                globals()[f\"hash202{i}_0{j}\"] = list()\n",
        "                globals()[f\"hash202{i}_0{j}\"].extend(\n",
        "                    [hash for row, hash in enumerate(df[\"hashtags\"]) if df[\"timestamp\"][row][0:7] == f\"202{i}-0{j}\" and hash != \"[]\"]\n",
        "                )\n",
        "\n",
        "            elif i == 1 and j > 5 and j >= 10 or i == 2 and j >= 10:\n",
        "                globals()[f\"hash202{i}_{j}\"] = list()\n",
        "                globals()[f\"hash202{i}_{j}\"].extend(\n",
        "                    [hash for row, hash in enumerate(df[\"hashtags\"]) if df[\"timestamp\"][row][0:7] == f\"202{i}-{j}\" and hash != \"[]\"]\n",
        "                )\n",
        "\n",
        "    # preprocessing the hashtags list from specific periods\n",
        "    i = 1\n",
        "    j = 6\n",
        "    for i in range(1, 3):\n",
        "        for j in range(1, 13):\n",
        "            if i == 1 and j > 5 and j < 10 or i == 2 and j < 10:\n",
        "                globals()[f\"all_hashtags_202{i}_0{j}\"] = list()\n",
        "                for s in globals()[f\"hash202{i}_0{j}\"]:\n",
        "                    bb = s.split(\", \")\n",
        "                    a = [re.search(r\"[\\[\\'\\\"]*(\\w*)[\\]\\']*\", i).group(1) for i in bb]\n",
        "                    globals()[f\"all_hashtags_202{i}_0{j}\"].extend(a)\n",
        "\n",
        "            elif i == 1 and j > 5 and j >= 10 or i == 2 and j >= 10:\n",
        "                globals()[f\"all_hashtags_202{i}_{j}\"] = list()\n",
        "                for s in globals()[f\"hash202{i}_{j}\"]:\n",
        "                    bb = s.split(\", \")\n",
        "                    a = [re.search(r\"[\\[\\'\\\"]*(\\w*)[\\]\\']*\", i).group(1) for i in bb]\n",
        "                    globals()[f\"all_hashtags_202{i}_{j}\"].extend(a)\n",
        "\n",
        "    def hashtag_set_finder():\n",
        "        hashtag_set = set()\n",
        "        for dt in date:\n",
        "            i, j = dt[3], dt[5:]\n",
        "            lis = globals()[f\"all_hashtags_202{i}_{j}\"]\n",
        "            for i in lis:\n",
        "                hashtag_set.add(i)\n",
        "        return hashtag_set\n",
        "\n",
        "    hashtags_in_DS = hashtag_set_finder()\n",
        "    hashtags_in_DS2 = {i.lower() for i in hashtags_in_DS}\n",
        "\n",
        "    def generate_hashtags(dataset):\n",
        "        listOfAllHashtags = list()\n",
        "        for row, hash in enumerate(dataset.hashtags):\n",
        "            if dataset[\"timestamp\"][row][0:10] >= \"2021-06-01\" and hash != \"[]\":\n",
        "                bb = hash.split(\", \")\n",
        "\n",
        "                a = [re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\", i).group(1) for i in bb]\n",
        "                listOfAllHashtags.extend(a)\n",
        "        listOfAllHashtagsL = [i.lower() for i in listOfAllHashtags]\n",
        "        return listOfAllHashtagsL\n",
        "\n",
        "    list_of_all_hashtags = generate_hashtags(df)\n",
        "    dic_of_mfh = {\n",
        "        i: list_of_all_hashtags.count(i) / len(list_of_all_hashtags) for i in hashtags_in_DS2 if list_of_all_hashtags.count(i) > 20\n",
        "    }\n",
        "    # input_hashtags = [i[0] for i in list_of_mfh]\n",
        "    return dic_of_mfh\n",
        "\n",
        "\n",
        "def hashtag_groups(df):\n",
        "    representative_hashtags = list()\n",
        "    model = SentenceTransformer(\"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\")\n",
        "    dic_of_mfh = extract_most_frequent_hashtags(df)\n",
        "    input_hashtags = list(dic_of_mfh.keys())\n",
        "    sentence = input_hashtags\n",
        "    clusterd_hashtags = list()\n",
        "    for hash in input_hashtags:\n",
        "        embedding = model.encode(hash)\n",
        "        sublist = {hash}\n",
        "        for j in range(len(input_hashtags)):\n",
        "            if j < len(input_hashtags) - 1:\n",
        "                if util.pytorch_cos_sim(embedding, model.encode(input_hashtags[j + 1])) > 0.5:\n",
        "                    sublist.add(input_hashtags[j + 1])\n",
        "                    input_hashtags.remove(input_hashtags[j + 1])\n",
        "        clusterd_hashtags.append(sublist)\n",
        "\n",
        "        clusterd_hashtags.sort(reverse=True, key=lambda i: len(i))\n",
        "    print(\"for each cluster a group name has been extracted based on the most frequent hashtag of that group or a Group Representative\\n\")\n",
        "    for i in clusterd_hashtags:\n",
        "        i = list(i)\n",
        "        group_n = i[0]\n",
        "        for j in i:\n",
        "            if dic_of_mfh[j] > dic_of_mfh[group_n]:\n",
        "                group_n = j\n",
        "        representative_hashtags.append(group_n)\n",
        "        print(f\"group name  : {group_n}\")\n",
        "        print(f\"{i}\\n\")\n",
        "    return representative_hashtags\n",
        "\n",
        "\n",
        "def tweet_clustering(df, tweet_id, rep_hashtags, model):\n",
        "  hashclusters = set()\n",
        "  tweet_hashtags = list()\n",
        "  hashtags = str(list(df[df.tweet_id == tweet_id].hashtags))\n",
        "  hashtags = hashtags.split(', ')\n",
        "  a = [re.search(r\"[\\[\\'\\\"]*(\\w*)[\\]\\']*\", i).group(1) for i in hashtags]\n",
        "  tweet_hashtags.extend(a)\n",
        "  #print(a)\n",
        "  if not tweet_hashtags[0]:\n",
        "    print(\"no hashtags found for this tweet\")\n",
        "    return []\n",
        "\n",
        "  for tw_hash in tweet_hashtags:\n",
        "    embedding = model.encode(tw_hash)\n",
        "    highestsim=0\n",
        "    hash=\"\"\n",
        "    for rep_hash in rep_hashtags:\n",
        "        sim=util.pytorch_cos_sim(embedding, model.encode(rep_hash))\n",
        "        if  sim> 0.5 and sim>highestsim:\n",
        "          highestsim=sim\n",
        "          hash=rep_hash\n",
        "    if hash != \"\":\n",
        "        hashclusters.add(hash)       \n",
        "  if not hashclusters:\n",
        "    print(f\"hashtags in tweet {tweet_id} are very infrequent and disimilar to the founded groups.\")\n",
        "    hashclusters.add(\"no_groups\")\n",
        "  return list(hashclusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pvBcXzhr4orH"
      },
      "outputs": [],
      "source": [
        "en_df, de_df, read_df = clean_data(\"twitter_data.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1uiu_PWd4orH",
        "outputId": "a0791417-f003-4bef-e38a-6f771d20ec5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>text</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>referenced_tweets</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>tweet_type</th>\n",
              "      <th>tweet_language</th>\n",
              "      <th>tweet_without_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-06-20T21:29:24.000Z</td>\n",
              "      <td>1406725899744157698</td>\n",
              "      <td>1406725899744157698</td>\n",
              "      <td>2981738470</td>\n",
              "      <td>Wenn Wirtschaftsjounalistinnen über  schreiben...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>4</td>\n",
              "      <td>[]</td>\n",
              "      <td>['IchbinHanna']</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>Wenn Wirtschaftsjounalistinnen schreiben amp D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-06-20T16:17:25.000Z</td>\n",
              "      <td>1406647386542325764</td>\n",
              "      <td>1406647386542325764</td>\n",
              "      <td>2981738470</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'quoted', 'id': '1406620276822061057'}]</td>\n",
              "      <td>['IchbinHanna', 'PeerReview']</td>\n",
              "      <td>original</td>\n",
              "      <td>nl</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-06-18T13:10:36.000Z</td>\n",
              "      <td>1405875593711964166</td>\n",
              "      <td>1405875593711964166</td>\n",
              "      <td>1132055796571877376</td>\n",
              "      <td>Thread  about the cruel  in German  The debate...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'quoted', 'id': '1405846267759054851'}]</td>\n",
              "      <td>['precarity', 'academia', 'IchbinHanna', 'Acad...</td>\n",
              "      <td>original</td>\n",
              "      <td>en</td>\n",
              "      <td>Thread cruel German The debate started days ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-06-18T10:21:31.000Z</td>\n",
              "      <td>1405833045224087555</td>\n",
              "      <td>1405833045224087555</td>\n",
              "      <td>242424959</td>\n",
              "      <td>Liebe  und Forsa  schön dass ihr euch per Umfr...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Mittelbau', 'IchBinHanna']</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>Liebe Forsa schön per Umfrage interessiert Die...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-06-18T08:35:29.000Z</td>\n",
              "      <td>1405806358335832065</td>\n",
              "      <td>1405806358335832065</td>\n",
              "      <td>1132055796571877376</td>\n",
              "      <td>Wichtiger Thread  zu  in der  der durch die  A...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'quoted', 'id': '1405494574533984264'}]</td>\n",
              "      <td>['Machtmissbrauch', 'Wissenschaft', 'prekär', ...</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>Wichtiger Thread Arbeitsverträge begünstigt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50901</th>\n",
              "      <td>2021-09-17T14:36:31.000Z</td>\n",
              "      <td>1438874511605125123</td>\n",
              "      <td>1438766234489794560</td>\n",
              "      <td>743696568939753475</td>\n",
              "      <td>I still cant get over how they titled their ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'replied_to', 'id': '143887433770928...</td>\n",
              "      <td>[]</td>\n",
              "      <td>original</td>\n",
              "      <td>en</td>\n",
              "      <td>I still cant get titled youngins Juniorprofess...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50902</th>\n",
              "      <td>2021-08-25T04:56:31.000Z</td>\n",
              "      <td>1430393629651177473</td>\n",
              "      <td>1430093188757401618</td>\n",
              "      <td>1134768684315160577</td>\n",
              "      <td>WissZeitVG Maximaldauer erreicht\\n\\nEinfach u...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'replied_to', 'id': '143009318875740...</td>\n",
              "      <td>[]</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>WissZeitVG Maximaldauer erreicht Einfach unfas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50903</th>\n",
              "      <td>2022-08-13T13:39:47.000Z</td>\n",
              "      <td>1558448232631762944</td>\n",
              "      <td>1558448232631762944</td>\n",
              "      <td>329984940</td>\n",
              "      <td>fellow profs  is OUR problem \\nSee this collec...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'type': 'quoted', 'id': '1556700609155317761'}]</td>\n",
              "      <td>['academicprecarity']</td>\n",
              "      <td>original</td>\n",
              "      <td>en</td>\n",
              "      <td>fellow profs OUR problem See collection info r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50904</th>\n",
              "      <td>2021-09-02T19:41:11.000Z</td>\n",
              "      <td>1433515364932067338</td>\n",
              "      <td>1433515364932067338</td>\n",
              "      <td>3005636663</td>\n",
              "      <td>beschließt neues Gesetz zur Stärkung der Berl...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Wissenschaft', 'Postdocs', 'Promotionsrecht'...</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>beschließt neues Gesetz Stärkung Berliner Daue...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50905</th>\n",
              "      <td>2022-06-28T08:32:01.000Z</td>\n",
              "      <td>1541700936128659458</td>\n",
              "      <td>1541700936128659458</td>\n",
              "      <td>78052485</td>\n",
              "      <td>Deutscher Hochschulverband DHV DHV legt Eckpun...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>['Hochschule', 'Wissenschaft']</td>\n",
              "      <td>original</td>\n",
              "      <td>de</td>\n",
              "      <td>Deutscher Hochschulverband DHV DHV legt Eckpun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50906 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      timestamp             tweet_id      conversation_id  \\\n",
              "0      2021-06-20T21:29:24.000Z  1406725899744157698  1406725899744157698   \n",
              "1      2021-06-20T16:17:25.000Z  1406647386542325764  1406647386542325764   \n",
              "2      2021-06-18T13:10:36.000Z  1405875593711964166  1405875593711964166   \n",
              "3      2021-06-18T10:21:31.000Z  1405833045224087555  1405833045224087555   \n",
              "4      2021-06-18T08:35:29.000Z  1405806358335832065  1405806358335832065   \n",
              "...                         ...                  ...                  ...   \n",
              "50901  2021-09-17T14:36:31.000Z  1438874511605125123  1438766234489794560   \n",
              "50902  2021-08-25T04:56:31.000Z  1430393629651177473  1430093188757401618   \n",
              "50903  2022-08-13T13:39:47.000Z  1558448232631762944  1558448232631762944   \n",
              "50904  2021-09-02T19:41:11.000Z  1433515364932067338  1433515364932067338   \n",
              "50905  2022-06-28T08:32:01.000Z  1541700936128659458  1541700936128659458   \n",
              "\n",
              "                 author_id                                               text  \\\n",
              "0               2981738470  Wenn Wirtschaftsjounalistinnen über  schreiben...   \n",
              "1               2981738470                                                      \n",
              "2      1132055796571877376  Thread  about the cruel  in German  The debate...   \n",
              "3                242424959  Liebe  und Forsa  schön dass ihr euch per Umfr...   \n",
              "4      1132055796571877376  Wichtiger Thread  zu  in der  der durch die  A...   \n",
              "...                    ...                                                ...   \n",
              "50901   743696568939753475    I still cant get over how they titled their ...   \n",
              "50902  1134768684315160577   WissZeitVG Maximaldauer erreicht\\n\\nEinfach u...   \n",
              "50903            329984940  fellow profs  is OUR problem \\nSee this collec...   \n",
              "50904           3005636663   beschließt neues Gesetz zur Stärkung der Berl...   \n",
              "50905             78052485  Deutscher Hochschulverband DHV DHV legt Eckpun...   \n",
              "\n",
              "       retweet_count  reply_count  like_count  quote_count  \\\n",
              "0                  9            1          62            4   \n",
              "1                  0            0           1            0   \n",
              "2                  5            0          19            0   \n",
              "3                  0            1           2            0   \n",
              "4                  2            0          13            0   \n",
              "...              ...          ...         ...          ...   \n",
              "50901              0            1           0            0   \n",
              "50902              0            0           0            0   \n",
              "50903              2            0           4            0   \n",
              "50904              1            0           6            0   \n",
              "50905              0            0           0            0   \n",
              "\n",
              "                                       referenced_tweets  \\\n",
              "0                                                     []   \n",
              "1      [{'type': 'quoted', 'id': '1406620276822061057'}]   \n",
              "2      [{'type': 'quoted', 'id': '1405846267759054851'}]   \n",
              "3                                                     []   \n",
              "4      [{'type': 'quoted', 'id': '1405494574533984264'}]   \n",
              "...                                                  ...   \n",
              "50901  [{'type': 'replied_to', 'id': '143887433770928...   \n",
              "50902  [{'type': 'replied_to', 'id': '143009318875740...   \n",
              "50903  [{'type': 'quoted', 'id': '1556700609155317761'}]   \n",
              "50904                                                 []   \n",
              "50905                                                 []   \n",
              "\n",
              "                                                hashtags tweet_type  \\\n",
              "0                                        ['IchbinHanna']   original   \n",
              "1                          ['IchbinHanna', 'PeerReview']   original   \n",
              "2      ['precarity', 'academia', 'IchbinHanna', 'Acad...   original   \n",
              "3                           ['Mittelbau', 'IchBinHanna']   original   \n",
              "4      ['Machtmissbrauch', 'Wissenschaft', 'prekär', ...   original   \n",
              "...                                                  ...        ...   \n",
              "50901                                                 []   original   \n",
              "50902                                                 []   original   \n",
              "50903                              ['academicprecarity']   original   \n",
              "50904  ['Wissenschaft', 'Postdocs', 'Promotionsrecht'...   original   \n",
              "50905                     ['Hochschule', 'Wissenschaft']   original   \n",
              "\n",
              "      tweet_language                            tweet_without_stopwords  \n",
              "0                 de  Wenn Wirtschaftsjounalistinnen schreiben amp D...  \n",
              "1                 nl                                                     \n",
              "2                 en  Thread cruel German The debate started days ag...  \n",
              "3                 de  Liebe Forsa schön per Umfrage interessiert Die...  \n",
              "4                 de        Wichtiger Thread Arbeitsverträge begünstigt  \n",
              "...              ...                                                ...  \n",
              "50901             en  I still cant get titled youngins Juniorprofess...  \n",
              "50902             de  WissZeitVG Maximaldauer erreicht Einfach unfas...  \n",
              "50903             en  fellow profs OUR problem See collection info r...  \n",
              "50904             de  beschließt neues Gesetz Stärkung Berliner Daue...  \n",
              "50905             de  Deutscher Hochschulverband DHV DHV legt Eckpun...  \n",
              "\n",
              "[50906 rows x 14 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "95IAzaUz4orI"
      },
      "outputs": [],
      "source": [
        "# Text Analysis LDA pipeline\n",
        "en_topics = 4\n",
        "de_topics = 5\n",
        "\n",
        "\n",
        "def run_lda(df, tagger: ht.HanoverTagger, language: str, n_topics: int):\n",
        "    \"\"\"\n",
        "    LDA Topic modeling text analysis pipeline.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data of specific language.\n",
        "                 `tagger (HanoverTagger)`: HanTa Tagger object to extraxt adjectives and noun from text.\n",
        "                 `language (str)`: Language of corpus. `english` or `german`\n",
        "                 `ntopics (int)`: number of topics\n",
        "        :return: \n",
        "                 `df (Dataframe)`: Dataframe object of tweet data of specific language with adjectives, noun and token columns.\n",
        "                 `dict_docs (Dict)`: Dictionary of lemantized noun from text.\n",
        "                 `corpus (Dict)`: Document token Matrix.\n",
        "                 `id2word (Dict)`: ID represenatation of words Dictionary.\n",
        "                 `lda_model (Gensim LDA Object)`: Lda model object.\n",
        "                 `doc_topic (Dict)`: document Topic matrix.\n",
        "                 `doc_topic_df (Dataframe)`: document Topic matrix Dataframe.\n",
        "                 `count_arr (array)`: Count of documents in each topic.\n",
        "                 `word_topic_dict (Dict)`: Word Topic matrix.\n",
        "                 `word_topic_df (Dataframe)`: Word Topic matrix Dataframe.\n",
        "    \"\"\"\n",
        "    df = extract_adj_noun(df, tagger)\n",
        "    if language == \"english\":\n",
        "        df, dict_docs = lementize_en_text(df)\n",
        "    elif language == \"german\":\n",
        "        df, dict_docs = lementize_de_text(df)\n",
        "    corpus, id2word = gen_doc_matrix(df)\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=42)\n",
        "    doc_topic, doc_topic_df, count_arr = fetch_doc_topic(df, corpus, lda_model, n_topics)\n",
        "    word_topic_dict, word_topic_df = fetch_word_topic(lda_model, id2word)\n",
        "    return df, dict_docs, corpus, id2word, lda_model, doc_topic, doc_topic_df, count_arr, word_topic_dict, word_topic_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AVTQCII54orI"
      },
      "outputs": [],
      "source": [
        "en_df, en_dict_docs, en_corpus, en_id2word, en_lda_model, en_doc_topic, en_doc_topic_df, en_count_arr, en_word_topic_dict, en_word_topic_df = run_lda(\n",
        "    en_df, en_tagger, \"english\", en_topics)\n",
        "\n",
        "de_df, de_dict_docs, de_corpus, de_id2word, de_lda_model, de_doc_topic, de_doc_topic_df, de_count_arr, de_word_topic_dict, de_word_topic_df = run_lda(\n",
        "    de_df, de_tagger, \"german\", de_topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#LDA for german and english text together\n",
        "en_de_arr= [en_df, de_df]\n",
        "en_de_lda_df = pd.concat(en_de_arr)\n",
        "\n",
        "#performing lda here on a combined dataset\n",
        "en_de_corpus, en_de_id2word = gen_doc_matrix(en_de_lda_df)\n",
        "en_de_lda_model = gensim.models.ldamodel.LdaModel(corpus=en_de_corpus, id2word=en_de_id2word, num_topics=3, random_state=42)\n",
        "en_de_doc_topic, en_de_doc_topic_df, en_de_count_arr = fetch_doc_topic(en_de_lda_df, en_de_corpus, en_de_lda_model, 3)\n",
        "en_de_word_topic_dict, en_de_word_topic_df = fetch_word_topic(en_de_lda_model, en_de_id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el131681844323416096384303952\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el131681844323416096384303952_data = {\"mdsDat\": {\"x\": [0.3022375782479405, -0.15334160940813643, -0.14889596883980422], \"y\": [-0.001389385328733422, -0.14099167982406913, 0.14238106515280255], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [67.0197924574673, 17.921851675563897, 15.058355866968798]}, \"tinfo\": {\"Term\": [\"Wissenschaft\", \"Dauerstelle\", \"Forschung\", \"Stellen\", \"Hochschule\", \"Jahre\", \"Anschlussverwendung\", \"Jahren\", \"Unis\", \"Lehre\", \"System\", \"Arbeit\", \"Zeit\", \"Uni\", \"Arbeitsbedingung\", \"Stelle\", \"Problem\", \"Leute\", \"Menschen\", \"Wissenschaftlerinnen\", \"b\", \"Bundesministerin\", \"Veranstaltung\", \"Frage\", \"Thema\", \"Bewegung\", \"Mittelbau\", \"Selbstwahrnehmung\", \"Jahr\", \"Morgen\", \"Dauerstelle\", \"Stellen\", \"Forschung\", \"Lehre\", \"Hochschule\", \"Wissenschaft\", \"Arbeitsbedingung\", \"Unis\", \"Jahre\", \"System\", \"Leute\", \"Stelle\", \"Jahren\", \"Frage\", \"Mittelbau\", \"Jahr\", \"Befristung\", \"Wissenschaftlerinnen\", \"Universit\\u00e4ten\", \"Professur\", \"Frau\", \"Zeit\", \"Daueraufgabe\", \"Politik\", \"Wissenschaftler\", \"Wisszeitvg\", \"Ende\", \"Menschen\", \"Debatte\", \"Arbeit\", \"Uni\", \"Problem\", \"Thema\", \"Karriere\", \"Geld\", \"Diskussion\", \"b\", \"e\", \"Berufserfahrung\", \"Tarifrunde\", \"Abschnitt\", \"Vollksverpetzer\", \"Appell\", \"Detail\", \"Berufungskommission\", \"Absage\", \"Tagungen\", \"Autorinnen\", \"Streit\", \"Leistungsprinzip\", \"d\", \"Auftritt\", \"Bewerbungsfrist\", \"Pr\\u00e4senzlehre\", \"Boden\", \"Schau\", \"Idealismus\", \"Meinungen\", \"Bewegungen\", \"Koalitionsverhandlung\", \"Inkompetenz\", \"Stellenausschreibung\", \"Willk\\u00fcr\", \"Feiertag\", \"Berufsgruppe\", \"Journaliste\", \"z\", \"Klimaschutz\", \"Plan\", \"Look\", \"Befristungsm\\u00f6glichkeit\", \"Berliner\", \"Reinigungskraft\", \"Lehrveranstaltung\", \"Student\", \"Handwerker\", \"Bank\", \"Mrd\", \"Gl\\u00fcckwunsch\", \"Anschlussverwendung\", \"Bewegung\", \"Selbstwahrnehmung\", \"Nachfrage\", \"Runde\", \"Herrn\", \"Labor\", \"Streitschrift\", \"Januar\", \"Au\\u00dfenwahrnehmung\", \"Pause\", \"Warnstreik\", \"Arbeitsalltag\", \"Einstieg\", \"Herren\", \"Version\", \"Bundesministerin\", \"Licht\", \"Mandat\", \"Pr\\u00fcfung\", \"Dienstleistung\", \"Fachverband\", \"Bev\\u00f6lkerung\", \"Wissenschaftskarriere\", \"Standbein\", \"Serie\", \"Ergebnissen\", \"Richtlinie\", \"Definition\", \"Lied\", \"Bibliotheke\", \"Gastbeitrag\", \"Privatdozent\", \"Veranstaltung\", \"Morgen\", \"Podiumsdiskussion\", \"Kundgebung\", \"Reihe\", \"Aspekte\", \"Transparenz\", \"Anschlusszusage\", \"Veranstaltungen\", \"Verhandlung\", \"R\\u00fccktritt\", \"Doktorvater\", \"Uhr\"], \"Freq\": [2915.0, 1694.0, 1631.0, 1424.0, 1402.0, 1356.0, 243.0, 1155.0, 1109.0, 1091.0, 1109.0, 1157.0, 1127.0, 1270.0, 1037.0, 887.0, 934.0, 821.0, 850.0, 832.0, 163.0, 146.0, 149.0, 730.0, 751.0, 125.0, 634.0, 116.0, 578.0, 113.0, 1693.2976213661966, 1423.450201300138, 1629.6433105339686, 1090.0465937941528, 1400.634421210972, 2911.6574530516455, 1036.1732857907537, 1108.0414365370311, 1354.3880641525743, 1108.108977795883, 820.0394060802518, 885.7424455743821, 1153.4955380575877, 729.3487064635833, 633.231735253781, 577.5998174374156, 453.156967957551, 830.6455093839676, 532.6205275380969, 497.4999300577381, 456.3125847118546, 1124.5602456372255, 426.09925396346284, 413.09636166288846, 513.4306159323773, 475.7215425626523, 548.6284915840315, 847.6257932781443, 461.1952348834888, 1154.5346448968162, 1262.7761977628957, 931.4422327096249, 749.685962347209, 505.82214455007716, 650.5352572131632, 517.2930502463198, 162.78273552337288, 94.47300982424741, 88.55693549800408, 88.24675792254047, 83.03402903491344, 69.19005351882328, 64.76628703861331, 53.75403420391302, 49.95910801727901, 54.93685208733922, 64.12589484904882, 45.52577913427853, 54.60833816903666, 43.35163896386466, 40.56807668694128, 59.3877884435508, 47.79348966219954, 46.52742478228452, 43.42836280638852, 42.37309252375003, 40.7656828696309, 49.534360387671946, 37.89101418126936, 42.085991235749226, 52.96930290760605, 65.72395296832009, 42.24714018478931, 41.26179554556714, 41.0633321388498, 49.111693932977104, 99.58092817620701, 52.70620389238143, 111.27780962757076, 59.680922388345614, 52.98849189038701, 73.19783714130321, 49.483678736065094, 67.79514374186469, 67.46775750864673, 51.891038851047526, 51.05660689747559, 57.54285897246737, 56.14308548057076, 241.77809026768807, 124.91057377801776, 115.24432882790522, 88.6186295850402, 76.66848403305167, 63.16272811050854, 62.27255666104607, 67.18528514663173, 69.9720039336524, 96.8055076801018, 55.969072500893255, 49.07282245074319, 47.84311988844023, 66.68956941375221, 43.199171460565644, 40.47847774769317, 144.0911929120742, 47.574016291688864, 42.96628112639399, 36.06723477453866, 38.323164656051546, 68.17193285658097, 38.63339237581541, 45.644652851473886, 40.85535836140958, 36.74393593555665, 38.84002212455116, 35.0700913040566, 59.64335131512273, 35.87870010212704, 63.50975839610405, 62.23224138563426, 49.93738039285031, 142.41351973526417, 108.61046123611027, 42.779448885599066, 41.58017439970322, 62.085605018938665, 53.793761615238246, 56.17261322212539, 46.7640016394553, 50.90860607521295, 54.22082929675019, 51.10925805501403, 53.51815642443939, 57.68866709335074], \"Total\": [2915.0, 1694.0, 1631.0, 1424.0, 1402.0, 1356.0, 243.0, 1155.0, 1109.0, 1091.0, 1109.0, 1157.0, 1127.0, 1270.0, 1037.0, 887.0, 934.0, 821.0, 850.0, 832.0, 163.0, 146.0, 149.0, 730.0, 751.0, 125.0, 634.0, 116.0, 578.0, 113.0, 1694.526323976084, 1424.8164550058389, 1631.3420505203294, 1091.2438177340478, 1402.254790930459, 2915.3921056401077, 1037.579152613588, 1109.5487475836385, 1356.5596335754742, 1109.889634250434, 821.3673530845338, 887.2619825342417, 1155.530463126633, 730.7432410134105, 634.489927825631, 578.7866625754795, 454.2160285879758, 832.5915068619654, 533.9251850278905, 498.72232390514716, 457.4407463923985, 1127.3477714608628, 427.1739646142522, 414.1564978551581, 514.8036266564654, 477.03958485340854, 550.1582139594822, 850.0318340290996, 462.52024212937425, 1157.9155035550627, 1270.4554064470512, 934.3281181674878, 751.9704091947709, 507.43127542779837, 661.5287970564948, 524.2408962524013, 163.5594806321065, 95.2041646651229, 89.31680525224856, 89.09847314915487, 83.91479702730095, 70.00554551659096, 65.60598701829565, 54.52405033774107, 50.70098533818539, 55.76067445844694, 65.10523016277982, 46.251522293357695, 55.49738293789188, 44.06335001746211, 41.256407229057075, 60.418533639605805, 48.656592237611264, 47.393899635535895, 44.254271389090725, 43.18554021979831, 41.54851718695189, 50.506568841057685, 38.64242880354497, 42.92979314963473, 54.057409426102396, 67.09531423263917, 43.163565233420115, 42.169910725534756, 41.97252159488945, 50.2303632369482, 103.18699021678695, 54.02629131215338, 116.18036807763602, 61.58278274833959, 54.59556187660218, 76.68243288151295, 50.91312595369768, 72.24095801229873, 72.50066652317182, 54.119481060430964, 53.45450474991209, 73.65072180875363, 142.7331348718025, 243.3443241605317, 125.80780304481156, 116.22922414316069, 89.47746354275665, 77.46859532309298, 63.997817461196945, 63.11603921405117, 68.22048407159943, 71.09839409417987, 98.39318431411166, 56.95842961315495, 49.9593222174989, 48.74725545082886, 67.96187998032158, 44.03071275874204, 41.259082622589816, 146.8781025887964, 48.49680186087828, 43.84576791063696, 36.89338029230466, 39.21090622114772, 69.75436923000913, 39.53227773835317, 46.716008320478956, 41.821247581948896, 37.61467769137945, 39.76260575764198, 35.91697978684567, 61.08456404499891, 36.75120061702783, 65.06807217667587, 63.93232499430874, 51.23917121031542, 149.25400401069558, 113.33179511607449, 43.85970862643787, 42.607854505549994, 64.16624206269373, 55.74442335840378, 59.33929148380375, 48.52134035665583, 53.8286517066234, 61.890539799269206, 56.89008627712628, 172.3791581501774, 381.8454038542083], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4655, -4.6391, -4.5038, -4.9059, -4.6552, -3.9234, -4.9566, -4.8896, -4.6888, -4.8895, -5.1906, -5.1135, -4.8494, -5.3078, -5.4491, -5.541, -5.7837, -5.1777, -5.6221, -5.6903, -5.7767, -4.8748, -5.8452, -5.8762, -5.6588, -5.7351, -5.5925, -5.1575, -5.7661, -4.8485, -4.7588, -5.0632, -5.2802, -5.6737, -5.4221, -5.6513, -5.4885, -6.0326, -6.0973, -6.1008, -6.1617, -6.3441, -6.4102, -6.5965, -6.6697, -6.5748, -6.4201, -6.7627, -6.5808, -6.8116, -6.878, -6.4968, -6.714, -6.7409, -6.8098, -6.8344, -6.8731, -6.6783, -6.9462, -6.8412, -6.6112, -6.3955, -6.8374, -6.861, -6.8658, -6.6868, -5.98, -6.6162, -5.8689, -6.4919, -6.6109, -6.2878, -6.6793, -6.3644, -6.3693, -6.6318, -6.648, -6.5284, -6.553, -4.9188, -5.5793, -5.6598, -5.9225, -6.0674, -6.2611, -6.2753, -6.1994, -6.1588, -5.8341, -6.3821, -6.5135, -6.5389, -6.2068, -6.641, -6.7061, -5.4364, -6.5446, -6.6464, -6.8215, -6.7608, -6.1848, -6.7527, -6.586, -6.6968, -6.8029, -6.7474, -6.8495, -6.3185, -6.8267, -6.2557, -6.276, -6.4961, -5.4481, -5.7191, -6.6508, -6.6792, -6.2783, -6.4217, -6.3784, -6.5617, -6.4768, -6.4138, -6.4729, -6.4268, -6.3518], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3995, 0.3992, 0.3991, 0.3991, 0.399, 0.3989, 0.3988, 0.3988, 0.3986, 0.3986, 0.3986, 0.3985, 0.3984, 0.3983, 0.3982, 0.3981, 0.3978, 0.3978, 0.3977, 0.3977, 0.3977, 0.3977, 0.3977, 0.3976, 0.3975, 0.3974, 0.3974, 0.3973, 0.3973, 0.3973, 0.3941, 0.3971, 0.3971, 0.397, 0.3834, 0.3868, 1.7144, 1.7114, 1.7106, 1.7095, 1.7086, 1.7074, 1.7063, 1.7049, 1.7044, 1.7043, 1.704, 1.7033, 1.703, 1.7029, 1.7023, 1.7019, 1.7013, 1.7007, 1.7003, 1.7002, 1.7001, 1.6997, 1.6995, 1.6993, 1.6988, 1.6985, 1.6977, 1.6974, 1.6972, 1.6966, 1.6836, 1.6944, 1.676, 1.6878, 1.6893, 1.6726, 1.6907, 1.6556, 1.6472, 1.6771, 1.6733, 1.4723, 0.7861, 1.8868, 1.8861, 1.8847, 1.8836, 1.8829, 1.8801, 1.8798, 1.8779, 1.8773, 1.877, 1.8757, 1.8753, 1.8745, 1.8743, 1.8742, 1.8741, 1.8741, 1.874, 1.873, 1.8706, 1.8703, 1.8703, 1.8702, 1.87, 1.8699, 1.8698, 1.8698, 1.8694, 1.8694, 1.8692, 1.869, 1.8663, 1.8675, 1.8463, 1.8507, 1.8683, 1.8688, 1.8603, 1.8576, 1.8384, 1.8563, 1.8375, 1.7609, 1.7861, 0.7236, 0.0033]}, \"token.table\": {\"Topic\": [2, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 3, 1, 2, 3, 1, 3, 1, 2, 2, 1, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 2, 3, 2, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 2, 3, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 2, 3, 1, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 2, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 1, 2, 3, 3, 2, 3, 2, 3, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 2, 2, 1, 2], \"Freq\": [0.986358227086837, 0.9890985015788892, 0.004109403428453546, 0.004109403428453546, 0.9944756296857581, 0.020609488374589528, 0.020609488374589528, 0.9686459536057078, 0.9907632360118802, 0.9974821102696082, 0.0008636208746923015, 0.001727241749384603, 0.9846708200509336, 0.9984780413045018, 0.0009637818931510636, 0.0009637818931510636, 0.01793901416058409, 0.9687067646715409, 0.01655121267862873, 0.9765215480390951, 0.9945618591370382, 0.010163305588399164, 0.9858406420747189, 0.03741499447721081, 0.9540823591688757, 0.018707497238605406, 0.9973227968379802, 0.0022015955780087864, 0.018316507159688494, 0.9707748794634901, 0.018316507159688494, 0.026081592939158973, 0.9519781422793024, 0.013040796469579487, 0.9964530163013127, 0.976829564726274, 0.9861741279087639, 0.025295785044781936, 0.9865356167464955, 0.9935790704132729, 0.9833750407664325, 0.9865055852163909, 0.015368520482438043, 0.9835853108760347, 0.9716576197117117, 0.022596688830504922, 0.013616733636594217, 0.006808366818297109, 0.9804048218347836, 0.9972517880032499, 0.0023409666385052815, 0.0023409666385052815, 0.9990992621628311, 0.0005901354176980692, 0.0005901354176980692, 0.9967131338460447, 0.0021620675354577976, 0.0021620675354577976, 0.0163707479235398, 0.0163707479235398, 0.9822448754123881, 0.9903886388759653, 0.02550310860861123, 0.9691181271272268, 0.9861878455035391, 0.0019075200106451434, 0.01144512006387086, 0.6845375117634461, 0.005801165353927509, 0.31326292911208553, 0.014714130925888908, 0.014714130925888908, 0.9858467720345567, 0.9978947620337312, 0.001817658947238126, 0.001817658947238126, 0.025149257221599716, 0.980821031642389, 0.014336019535960315, 0.9748493284453015, 0.9722572159768327, 0.9991773334599563, 0.0006129922291165376, 0.0006129922291165376, 0.99761442745472, 0.0013684697221601097, 0.0013684697221601097, 0.9968504196362896, 0.00218607548165853, 0.00218607548165853, 0.015641539707636473, 0.9697754618734614, 0.9840841440261661, 0.015116499908236037, 0.0015116499908236038, 0.6025230236640003, 0.3923405735486514, 0.007006081670511631, 0.03695526935608931, 0.960837003258322, 0.018477634678044655, 0.9765910498794412, 0.984408570467236, 0.9991051619587433, 0.0007131371605701237, 0.0007131371605701237, 0.9867981525192877, 0.01849885169519677, 0.9804391398454286, 0.9986408419088668, 0.001727752321641638, 0.9981131433428195, 0.0007371588946401917, 0.0007371588946401917, 0.9978101285882276, 0.0008654034072751324, 0.0008654034072751324, 0.014065015289590912, 0.014065015289590912, 0.9845510702713638, 0.01990827729600062, 0.9755055875040303, 0.9971793708880248, 0.001970710219146294, 0.001970710219146294, 0.018509506681148903, 0.9810038541008919, 0.9783415413534867, 0.023293846222702065, 0.02346985107803873, 0.9857337452776268, 0.015843833238784345, 0.9823176608046293, 0.9988601834770249, 0.0009163854894284632, 0.0009163854894284632, 0.055370251309776596, 0.9412942722662021, 0.013842562827444149, 0.9758676991867229, 0.9983352721782782, 0.0012174820392418027, 0.0012174820392418027, 0.020619916399202534, 0.9897559871617216, 0.027209995407243184, 0.9795598346607546, 0.016238304853590953, 0.9742982912154572, 0.016238304853590953, 0.9807103866361575, 0.019799404769446194, 0.9899702384723096, 0.997609696545753, 0.002352853057890927, 0.0011764265289454635, 0.9976517707211888, 0.00157606914805875, 0.00157606914805875, 0.03529459668315672, 0.00882364917078918, 0.9617777596160207, 0.2172415912168066, 0.787500768160924, 0.9946638681534765, 0.017556663812392802, 0.9831731734939969, 0.03442922471485933, 0.9554109858373464, 0.0227999690676745, 0.9803986699100036, 0.9972075824932185, 0.002414546204584064, 0.019516318792421862, 0.019516318792421862, 0.975815939621093, 0.9964379556787659, 0.00214057562981475, 0.001070287814907375, 0.9965465273508095, 0.0020051237974865383, 0.9916888114595966, 0.9757848078645424, 0.03116903742073436, 0.9662401600427653, 0.0196413003772237, 0.9624237184839614, 0.0196413003772237, 0.027841984652792064, 0.9744694628477223, 0.9939511576124668, 0.08788877513111354, 0.01757775502622271, 0.8964655063373581, 0.9725477506182775, 0.00860368816338557, 0.9894241387893405, 0.026585366707241, 0.9836585681679171, 0.9803629104956838, 0.99857766639495, 0.0011270628288882055, 0.0011270628288882055, 0.9987251305250883, 0.0007018447860330909, 0.0007018447860330909, 0.01490417045417965, 0.9836752499758569, 0.9910377226535437, 0.014658353918310961, 0.014658353918310961, 0.9821097125268344, 0.06896488321803164, 0.9241294351216239, 0.9982974575199901, 0.0009009904851263449, 0.0009009904851263449, 0.015359749093886049, 0.9830239420087071, 0.9876712460906487, 0.9973796718984184, 0.0013298395625312245, 0.0013298395625312245, 0.05055672093454013, 0.01685224031151338, 0.9437254574447491, 0.8458920723930569, 0.002618860905241662, 0.1518939325040164, 0.9941317055213288, 0.004722715940718902, 0.0007871193234531503, 0.9986041644524305, 0.0009012672964372117, 0.0009012672964372117, 0.9982672009977537, 0.0018729215778569488, 0.0018729215778569488, 0.04019992655989342, 0.9513982619174776, 0.03715493397271751, 0.9474508163042965, 0.11310290753163951, 0.016157558218805646, 0.8725081438155048, 0.96948350417514, 0.9856362019727045, 0.014284582637285572, 0.02001628436123453, 0.980797933700492, 0.9730428840359274, 0.9988364839043279, 0.0006860140686156098, 0.0006860140686156098, 0.9964964763978459, 0.0019424882580854697, 0.9980884901553179, 0.0012010691818956894, 0.0012010691818956894, 0.021405938477017283, 0.984673169942795, 0.9978207576762671, 0.0020962620959585443, 0.0020962620959585443, 0.9979174381497021, 0.0008870377227997352, 0.0008870377227997352, 0.9965793445299271, 0.9937850325251665, 0.9873517648166075, 0.029073432548979858, 0.9691144182993287], \"Term\": [\"Absage\", \"Abschnitt\", \"Anschlussverwendung\", \"Anschlussverwendung\", \"Anschlussverwendung\", \"Anschlusszusage\", \"Anschlusszusage\", \"Anschlusszusage\", \"Appell\", \"Arbeit\", \"Arbeit\", \"Arbeit\", \"Arbeitsalltag\", \"Arbeitsbedingung\", \"Arbeitsbedingung\", \"Arbeitsbedingung\", \"Aspekte\", \"Aspekte\", \"Auftritt\", \"Auftritt\", \"Autorinnen\", \"Au\\u00dfenwahrnehmung\", \"Au\\u00dfenwahrnehmung\", \"Bank\", \"Bank\", \"Bank\", \"Befristung\", \"Befristung\", \"Befristungsm\\u00f6glichkeit\", \"Befristungsm\\u00f6glichkeit\", \"Befristungsm\\u00f6glichkeit\", \"Berliner\", \"Berliner\", \"Berliner\", \"Berufserfahrung\", \"Berufsgruppe\", \"Berufungskommission\", \"Bev\\u00f6lkerung\", \"Bev\\u00f6lkerung\", \"Bewegung\", \"Bewegungen\", \"Bewerbungsfrist\", \"Bibliotheke\", \"Bibliotheke\", \"Boden\", \"Boden\", \"Bundesministerin\", \"Bundesministerin\", \"Bundesministerin\", \"Daueraufgabe\", \"Daueraufgabe\", \"Daueraufgabe\", \"Dauerstelle\", \"Dauerstelle\", \"Dauerstelle\", \"Debatte\", \"Debatte\", \"Debatte\", \"Definition\", \"Definition\", \"Definition\", \"Detail\", \"Dienstleistung\", \"Dienstleistung\", \"Diskussion\", \"Diskussion\", \"Diskussion\", \"Doktorvater\", \"Doktorvater\", \"Doktorvater\", \"Einstieg\", \"Einstieg\", \"Einstieg\", \"Ende\", \"Ende\", \"Ende\", \"Ergebnissen\", \"Ergebnissen\", \"Fachverband\", \"Fachverband\", \"Feiertag\", \"Forschung\", \"Forschung\", \"Forschung\", \"Frage\", \"Frage\", \"Frage\", \"Frau\", \"Frau\", \"Frau\", \"Gastbeitrag\", \"Gastbeitrag\", \"Geld\", \"Geld\", \"Geld\", \"Gl\\u00fcckwunsch\", \"Gl\\u00fcckwunsch\", \"Gl\\u00fcckwunsch\", \"Handwerker\", \"Handwerker\", \"Handwerker\", \"Herren\", \"Herrn\", \"Hochschule\", \"Hochschule\", \"Hochschule\", \"Idealismus\", \"Inkompetenz\", \"Inkompetenz\", \"Jahr\", \"Jahr\", \"Jahre\", \"Jahre\", \"Jahre\", \"Jahren\", \"Jahren\", \"Jahren\", \"Januar\", \"Januar\", \"Januar\", \"Journaliste\", \"Journaliste\", \"Karriere\", \"Karriere\", \"Karriere\", \"Klimaschutz\", \"Klimaschutz\", \"Koalitionsverhandlung\", \"Koalitionsverhandlung\", \"Kundgebung\", \"Kundgebung\", \"Labor\", \"Labor\", \"Lehre\", \"Lehre\", \"Lehre\", \"Lehrveranstaltung\", \"Lehrveranstaltung\", \"Lehrveranstaltung\", \"Leistungsprinzip\", \"Leute\", \"Leute\", \"Leute\", \"Licht\", \"Licht\", \"Lied\", \"Lied\", \"Look\", \"Look\", \"Look\", \"Mandat\", \"Meinungen\", \"Meinungen\", \"Menschen\", \"Menschen\", \"Menschen\", \"Mittelbau\", \"Mittelbau\", \"Mittelbau\", \"Morgen\", \"Morgen\", \"Morgen\", \"Mrd\", \"Mrd\", \"Nachfrage\", \"Pause\", \"Pause\", \"Plan\", \"Plan\", \"Podiumsdiskussion\", \"Podiumsdiskussion\", \"Politik\", \"Politik\", \"Privatdozent\", \"Privatdozent\", \"Privatdozent\", \"Problem\", \"Problem\", \"Problem\", \"Professur\", \"Professur\", \"Pr\\u00e4senzlehre\", \"Pr\\u00fcfung\", \"Reihe\", \"Reihe\", \"Reinigungskraft\", \"Reinigungskraft\", \"Reinigungskraft\", \"Richtlinie\", \"Richtlinie\", \"Runde\", \"R\\u00fccktritt\", \"R\\u00fccktritt\", \"R\\u00fccktritt\", \"Schau\", \"Selbstwahrnehmung\", \"Selbstwahrnehmung\", \"Serie\", \"Serie\", \"Standbein\", \"Stelle\", \"Stelle\", \"Stelle\", \"Stellen\", \"Stellen\", \"Stellen\", \"Stellenausschreibung\", \"Stellenausschreibung\", \"Streit\", \"Streitschrift\", \"Streitschrift\", \"Streitschrift\", \"Student\", \"Student\", \"System\", \"System\", \"System\", \"Tagungen\", \"Tagungen\", \"Tarifrunde\", \"Thema\", \"Thema\", \"Thema\", \"Transparenz\", \"Transparenz\", \"Transparenz\", \"Uhr\", \"Uhr\", \"Uhr\", \"Uni\", \"Uni\", \"Uni\", \"Unis\", \"Unis\", \"Unis\", \"Universit\\u00e4ten\", \"Universit\\u00e4ten\", \"Universit\\u00e4ten\", \"Veranstaltung\", \"Veranstaltung\", \"Veranstaltungen\", \"Veranstaltungen\", \"Verhandlung\", \"Verhandlung\", \"Verhandlung\", \"Version\", \"Vollksverpetzer\", \"Vollksverpetzer\", \"Warnstreik\", \"Warnstreik\", \"Willk\\u00fcr\", \"Wissenschaft\", \"Wissenschaft\", \"Wissenschaft\", \"Wissenschaftler\", \"Wissenschaftler\", \"Wissenschaftlerinnen\", \"Wissenschaftlerinnen\", \"Wissenschaftlerinnen\", \"Wissenschaftskarriere\", \"Wissenschaftskarriere\", \"Wisszeitvg\", \"Wisszeitvg\", \"Wisszeitvg\", \"Zeit\", \"Zeit\", \"Zeit\", \"b\", \"d\", \"e\", \"z\", \"z\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el131681844323416096384303952\", ldavis_el131681844323416096384303952_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el131681844323416096384303952\", ldavis_el131681844323416096384303952_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el131681844323416096384303952\", ldavis_el131681844323416096384303952_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "1      0.302238 -0.001389       1        1  67.019792\n",
              "0     -0.153342 -0.140992       2        1  17.921852\n",
              "2     -0.148896  0.142381       3        1  15.058356, topic_info=                  Term         Freq        Total Category  logprob  loglift\n",
              "3745      Wissenschaft  2915.000000  2915.000000  Default  30.0000  30.0000\n",
              "3747       Dauerstelle  1694.000000  1694.000000  Default  29.0000  29.0000\n",
              "3860         Forschung  1631.000000  1631.000000  Default  28.0000  28.0000\n",
              "3888           Stellen  1424.000000  1424.000000  Default  27.0000  27.0000\n",
              "3896        Hochschule  1402.000000  1402.000000  Default  26.0000  26.0000\n",
              "...                ...          ...          ...      ...      ...      ...\n",
              "9586   Veranstaltungen    50.908606    53.828652   Topic3  -6.4768   1.8375\n",
              "8081       Verhandlung    54.220829    61.890540   Topic3  -6.4138   1.7609\n",
              "12632        Rücktritt    51.109258    56.890086   Topic3  -6.4729   1.7861\n",
              "8029       Doktorvater    53.518156   172.379158   Topic3  -6.4268   0.7236\n",
              "5350               Uhr    57.688667   381.845404   Topic3  -6.3518   0.0033\n",
              "\n",
              "[155 rows x 6 columns], token_table=       Topic      Freq                 Term\n",
              "term                                       \n",
              "10030      2  0.986358               Absage\n",
              "14166      2  0.989099            Abschnitt\n",
              "12460      1  0.004109  Anschlussverwendung\n",
              "12460      2  0.004109  Anschlussverwendung\n",
              "12460      3  0.994476  Anschlussverwendung\n",
              "...      ...       ...                  ...\n",
              "201        2  0.996579                    b\n",
              "1361       2  0.993785                    d\n",
              "1046       2  0.987352                    e\n",
              "7465       1  0.029073                    z\n",
              "7465       2  0.969114                    z\n",
              "\n",
              "[272 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 3])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_de_vis = pyLDAvis.gensim_models.prepare(en_de_lda_model,\n",
        "                                     en_de_corpus,\n",
        "                                     dictionary=en_de_lda_model.id2word)\n",
        "en_de_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet: Like so many  Ive learned that I am a good enough researcher to stay in academia but what the  does is not to select the best candidates It also does not train researchers to become the best candidates deserving of permanent jobs\n",
            "Hashtags: ['IchbinHanna', 'WissZeitVG']\n",
            "Topic_Probability: [(0, 0.22442107), (1, 0.086784974), (2, 0.68879396)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: For those who dont read German\n",
            "\n",
            "The response by the ministry of education to the outpouring of stories and discussions regarding academic precarity  is unbelievably patronising and disrespectful\n",
            "Hashtags: ['IchBinHanna']\n",
            "Topic_Probability: [(0, 0.4263605), (1, 0.044729613), (2, 0.52890986)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: So  proudly explains the advantages of the  to early carrer researchers as if they were preschool children This law is a great obstacle to many brilliant young minds and threatens their future  nothing to be proud of \n",
            "\n",
            "Hashtags: ['WissZeitVG', 'IchbinHanna']\n",
            "Topic_Probability: [(0, 0.7958174), (1, 0.08789334), (2, 0.1162893)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: German academic system has a problem of unintended consequences Temporary job limit and scarcity of permanent positions break up families intensify psychological and physical problems Follow  for experiences of brilliant people at the end of their ropes\n",
            "Hashtags: ['IchbinHanna']\n",
            "Topic_Probability: [(0, 0.70852596), (1, 0.050345458), (2, 0.2411286)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Topic  1\n",
            "Most Probable Word List:  ['b', 'e', 'd', 'Plan', 'Idealismus', 'Aufsatz', 'Mieten', 'Berufserfahrung', 'Räte', 'Glückwunsch', 'Student', 'z', 'Note', 'Freund', 'Wenns', 'Feiertag', 'Seltenheit', 'Bubble', 'Reflexion', 'Geburtstag', 'Mrd', 'Stellenausschreibung', 'Intelligenz', 'Boden', 'Krankenhaus', 'Boomer', 'Praktika', 'Journaliste', 'Ausblick', 'Assistenten', 'Dissertationen', 'Freiheiten', 'Motto', 'Code', 'Beobachtung', 'Absage', 'Tagungen', 'Absolventinnen', 'Meinungen', 'Auftritt', 'Befristungsregel', 'Schau', 'Klimaschutz', 'Steuer', 'Bedenken', 'Erfindung', 'Lehrveranstaltung', 'Detail', 'Leistungsprinzip', 'Berater', 'Jura', 'Unfähigkeit', 'Handwerker', 'Rande', 'Güte', 'Bio', 'Berufsgruppe', 'Reinigungskraft', 'Ichs', 'Berufungskommission', 'Beweis', 'Körper', 'Schülern', 'Einsamkeit', 'Evaluationen', 'Autorinnen', 'Willkür', 'Gegenseite', 'Hochschulverband', 'Onlinetagung', 'Präsenzlehre', 'Budget', 'Bewerbungsfrist', 'Hörsaal', 'Termine', 'Zusammenbruch', 'Mär', 'Hirn', 'Ausfinanzierung', 'Ohr', 'Referentin', 'Mischung', 'Bank', 'Abschnitt', 'Verfassungsbeschwerde', 'Tarifrunde', 'Postfach', 'Besuch', 'Ausschuss', 'Teilnehmerinnen', 'Appell', 'Streit', 'Bildungsbereich', 'Selbstoptimierung', 'Gebiete', 'Befristungsmöglichkeit', 'Semesterbeitrag', 'Bewegungen', 'Gründung', 'Hochschulvertrag', 'Inkompetenz', 'Sitte', 'Koalitionsverhandlung', 'Berliner', 'Streikcafé', 'Fan', 'Bundeswehr', 'Dienstes', 'Sondervermögen', 'Vollksverpetzer', 'Antisemitismus', 'Forschungsdatum', 'Look', 'Region']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  2\n",
            "Most Probable Word List:  ['Arbeit', 'Wissenschaft', 'Dauerstelle', 'Menschen', 'Wissenschaftlerinnen', 'System', 'Ende', 'Jahren', 'Professur', 'Zeit', 'Debatte', 'Diskussion', 'Geld', 'Thema', 'Forschung', 'Uni', 'Stellen', 'Stelle', 'Hochschule', 'Jahre', 'Problem', 'Lehre', 'Arbeitsbedingung', 'Befristung', 'Mittelbau', 'Frage', 'Leute', 'Unis', 'Jahr', 'Universitäten', 'Wissenschaftler', 'Wisszeitvg', 'Frau', 'Karriere']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  3\n",
            "Most Probable Word List:  ['Bewegung', 'Gastbeitrag', 'Uhr', 'Serie', 'Blätter', 'Rechte', 'Reihe', 'Herren', 'Nachfrage', 'Aspekte', 'Fachverband', 'Januar', 'Vorbereitung', 'Morgen', 'Mai', 'Veranstaltung', 'Abgabe', 'Kolumne', 'Labor', 'Doktorvater', 'Verhandlung', 'Rezension', 'Ergebnissen', 'Version', 'Pause', 'Runde', 'Licht', 'Betreuerinnen', 'Podiumsdiskussion', 'Dienstleistung', 'Herrn', 'Veranstaltungen', 'Einstieg', 'Standbein', 'Transparenz', 'Wissenschaftskarriere', 'Bevölkerung', 'Definition', 'Warnstreik', 'Bibliotheke', 'Arbeitsalltag', 'Anschlussverwendung', 'Bundesministerin', 'Außenwahrnehmung', 'Selbstwahrnehmung', 'Rücktritt', 'Mandat', 'Streitschrift', 'Anschlusszusage', 'Dezember', 'Interpretation', 'Kundgebung', 'Privatdozent', 'Grundsatz']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  4\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "single positional indexer is out-of-bounds",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m print_beauty(en_de_doc_topic_df, en_de_word_topic_df)\n",
            "Cell \u001b[1;32mIn[2], line 402\u001b[0m, in \u001b[0;36mprint_beauty\u001b[1;34m(df, word_topic_df)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[0;32m    400\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTopic \u001b[39m\u001b[39m\"\u001b[39m, (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    401\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMost Probable Word List: \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> 402\u001b[0m           word_topic_df\u001b[39m.\u001b[39;49miloc[i]\u001b[39m.\u001b[39mmost_prob_words)\n\u001b[0;32m    403\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------------------------------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Softwares\\anaconda3\\envs\\dbse-project\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
            "File \u001b[1;32mc:\\Softwares\\anaconda3\\envs\\dbse-project\\lib\\site-packages\\pandas\\core\\indexing.py:1625\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index by location index with a non-integer key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1624\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1625\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[0;32m   1627\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_ixs(key, axis\u001b[39m=\u001b[39maxis)\n",
            "File \u001b[1;32mc:\\Softwares\\anaconda3\\envs\\dbse-project\\lib\\site-packages\\pandas\\core\\indexing.py:1557\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ],
      "source": [
        "print_beauty(en_de_doc_topic_df, en_de_word_topic_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YTXIJKt4orJ",
        "outputId": "f6e8cf21-6e49-4a84-e672-e75ccc6d8305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet: Like so many  Ive learned that I am a good enough researcher to stay in academia but what the  does is not to select the best candidates It also does not train researchers to become the best candidates deserving of permanent jobs\n",
            "Hashtags: ['IchbinHanna', 'WissZeitVG']\n",
            "Topic_Probability: [(0, 0.064213924), (1, 0.06291898), (2, 0.80872524), (3, 0.06414183)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: For those who dont read German\n",
            "\n",
            "The response by the ministry of education to the outpouring of stories and discussions regarding academic precarity  is unbelievably patronising and disrespectful\n",
            "Hashtags: ['IchBinHanna']\n",
            "Topic_Probability: [(0, 0.73760694), (1, 0.033181116), (2, 0.19639003), (3, 0.032821883)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: So  proudly explains the advantages of the  to early carrer researchers as if they were preschool children This law is a great obstacle to many brilliant young minds and threatens their future  nothing to be proud of \n",
            "\n",
            "Hashtags: ['WissZeitVG', 'IchbinHanna']\n",
            "Topic_Probability: [(0, 0.06581284), (1, 0.062984236), (2, 0.0637826), (3, 0.8074204)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: German academic system has a problem of unintended consequences Temporary job limit and scarcity of permanent positions break up families intensify psychological and physical problems Follow  for experiences of brilliant people at the end of their ropes\n",
            "Hashtags: ['IchbinHanna']\n",
            "Topic_Probability: [(0, 0.039656598), (1, 0.036251377), (2, 0.8867709), (3, 0.037321143)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Topic  1\n",
            "Most Probable Word List:  ['debate', 'research', 'video', 'life', 'academia', 'law', 'end', 'problem', 'system', 'year', 'change', 'amp', 'time', 'work', 'student', 'brain', 'employment', 'career', 'contract', 'university', 'position', 'science', 'professor', 'movement', 'world', 'youre', 'link', 'program', 'level', 'page', 'music', 'click', 'distribution']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  2\n",
            "Most Probable Word List:  ['discussion', 'audience', 'academia', 'system', 'thing', 'amp', 'time', 'pressure', 'lot', 'share', 'work', 'goal', 'tenure', 'track', 'science', 'attention', 'cant', 'day', 'part', 'respect', 'market', 'case', 'feature', 'link', 'story', 'love', 'week', 'anxiety', 'society', 'tag', 'hand', 'night', 'talk', 'music', 'play', 'reward', 'airdrop', 'telegram', 'channel', 'email', 'joy', 'connection', 'message', 'event', 'guide', 'appreciation', 'shape', 'battle', 'shark']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  3\n",
            "Most Probable Word List:  ['education', 'research', 'job', 'situation', 'academia', 'stay', 'dont', 'system', 'amp', 'time', 'lot', 'work', 'topic', 'student', 'security', 'career', 'u', 'way', 'lack', 'position', 'science', 'need', 'postdoc', 'sense', 'side', 'perspective', 'freedom', 'watch', 'stop', 'freelancer']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  4\n",
            "Most Probable Word List:  ['thread', 'post', 'situation', 'academia', 'dont', 'law', 'end', 'article', 'year', 'amp', 'time', 'u', 'professor', 'action', 'uk', 'seminar', 'casualisation', 'chef']\n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_beauty(en_doc_topic_df, en_word_topic_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekM6B5VL4orJ",
        "outputId": "838e5ec2-da00-4ffd-d468-1fecdc66ca48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet: Wichtiger Thread  zur Fehlwahrnehmung wissenschaftlicher Arbeit in der Öffentlichkeit Wissenschaft als von der Allgemeinheit bezahltes Hobby \n",
            "   \n",
            "Hashtags: ['IchbinHanna', 'Wissenschaft', 'WissZeitVG']\n",
            "Topic_Probability: [(0, 0.60016865), (1, 0.025433093), (2, 0.025454503), (3, 0.025494363), (4, 0.3234494)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet:       \n",
            "Hashtags: ['IchbinHanna', 'Ausbeutung', 'Arbeitsrecht']\n",
            "Topic_Probability: [(0, 0.2), (1, 0.2), (2, 0.2), (3, 0.2), (4, 0.2)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: Letzter Punkt Gerade die VWL mit ihren Engführungen amp der Marginalisierung heterodoxer Strömungen zeigt dass es auch strukturelle Probleme amp Pfadabhängigkeiten in der Selbstverwaltung der Wissenschaften gibt die mit mehr Dauerstellen nur noch weiter zementiert werden \n",
            "Hashtags: []\n",
            "Topic_Probability: [(0, 0.017720742), (1, 0.017705793), (2, 0.018131433), (3, 0.9287428), (4, 0.017699191)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Tweet: Ein Erklärungsansatz von ihm ist dass das  notwendig wurde um Schluss zu machen mit jenen verbeamteten Akademischen OberRäten die es sich  wenig motivierten Zaunkönigen gleich  im Windschatten ihrer jeweiligen Chefs gemütlich gemacht hatten \n",
            "Hashtags: ['WissZeitVG']\n",
            "Topic_Probability: [(0, 0.54994375), (1, 0.34242845), (2, 0.036056798), (3, 0.0359407), (4, 0.035630263)]\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Topic  1\n",
            "Most Probable Word List:  ['Amp', 'Arbeit', 'Wissenschaft', 'Menschen', 'Wissenschaftlerinnen', 'Zeit', 'Thema', 'Forschung', 'Hochschule', 'Bildung', 'Arbeitsbedingung', 'Erfahrungen', 'Hashtag', 'Professorinnen', 'Stunden', 'Fall', 'Unterstützung', 'Gesellschaft', 'Buch', 'Teil']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  2\n",
            "Most Probable Word List:  ['Amp', 'Wissenschaft', 'Dauerstelle', 'Menschen', 'Wissenschaftlerinnen', 'System', 'Ende', 'Jahren', 'Professur', 'Dank', 'Zeit', 'Geld', 'Thema', 'Leben', 'Unsicherheit', 'Uni', 'Stellen', 'Stelle', 'Prozent', 'Jahre', 'Gegenteil', 'Wenn', 'Glück', 'Track', 'Problem', 'Arbeitsbedingung', 'Anfang', 'Sicherheit', 'Perspektiven', 'Vertrag', 'Leute', 'Unis', 'Jahr', 'Universitäten', 'Drittmittel', 'Professor', 'Karriere', 'Doktorarbeit', 'Verträge', 'Perspektive', 'Realität', 'Eindruck', 'Weg', 'Hand', 'Arbeiten', 'Beitrag', 'Antrag', 'Tage', 'Aussage', 'Richtung', 'Daten', 'Entscheidung', 'Ach', 'Staat', 'Karriereweg', 'Herausforderung', 'Institut']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  3\n",
            "Most Probable Word List:  ['Arbeit', 'Dauerstelle', 'Wissenschaftlerinnen', 'Zeit', 'Geld', 'Forschung', 'Stellen', 'Hochschule', 'Artikel', 'Politik', 'Beispiel', 'Gesetz', 'Gespräch', 'Mittelbau', 'Zb', 'Unis', 'Wissenschaftler', 'Woche', 'Druck', 'Frau', 'Reform', 'Daueraufgabe', 'Ministerin', 'Bundestag', 'Vorschlag']\n",
            "-------------------------------------------------------\n",
            "\n",
            "Topic  4\n",
            "Most Probable Word List:  ['Amp', 'Arbeit', 'Wissenschaft', 'Dauerstelle', 'Probleme', 'Menschen', 'Wissenschaftssystem', 'System', 'Ende', 'Jahren', 'Zeit', 'Forschung', 'Uni', 'Stellen', 'Stelle', 'Hochschule', 'Situation', 'Jahre', 'Wirtschaft', 'Studium', 'Problem', 'Lehre', 'Arbeitsbedingung', 'Befristung', 'Frauen', 'Frage', 'Leute', 'Unis', 'Jahr', 'Verwaltung', 'Job', 'Monate']\n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_beauty(de_doc_topic_df, de_word_topic_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gn9E-LD04orK"
      },
      "outputs": [],
      "source": [
        "# Text Analysis BERTopic pipeline\n",
        "def run_bert(df, model_name: str =\"BERTopic_model\", choice: str= \"train\", embed: str =\"default\"):\n",
        "    \"\"\"\n",
        "    Method to run BERTopic topic modeling pipeline.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: tweet dataset dataframe.\n",
        "                 `choice (str)`: Train or load BERTopic model. `train` or `load`\n",
        "                 `model_name (str)`: File name of the BERTopic model to be loaded or saved as after training.\n",
        "                 `embed (str)`: Sentence Embedding name to be used. `roberta` or `default`\n",
        "        :return: \n",
        "                 `model (BERTopic model)`: Document fitted BERTopic model object.\n",
        "                 `topics (Dict)`: Topic matrix.\n",
        "                 `probabilities (Dict)`: Document topic probabilities matrix.\n",
        "    \"\"\"\n",
        "    model = topics = probabilities = None\n",
        "    if choice == \"load\":\n",
        "        model = load_bert_topic_model(model_name)\n",
        "    elif choice == 'train':\n",
        "        model, topics, probabilities = run_bert_topic_model(df, method=embed)\n",
        "\n",
        "    return model, topics, probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kGThVOHt4orK"
      },
      "outputs": [],
      "source": [
        "# bert_50, topics_50, prob_50 = run_bert(read_df, \"BERTopic_50\", \"load\", \"default\")\n",
        "bert_roberta_50, topics_roberta_50, prob_roberta_50 = run_bert(read_df, \"BERTopic_Roberta_50\",  \"load\", \"default\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kSVS23xv4orK"
      },
      "outputs": [],
      "source": [
        "def create_topics_df(df, representative_hashtags, bert):\n",
        "    \"\"\"\n",
        "    Method to create new dataframe with topics extracted from text and hashtags.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: tweet dataset dataframe.\n",
        "                 `representative_hashtags (Dict)`: Hashtag Cluster Matrix with a representative name for each cluster.\n",
        "                 `bert (BERTopic model)`: Document fitted BERTopic model object with defualt embedding.\n",
        "        :return: \n",
        "                 `df (Dataframe)`: New tweet dataset dataframe with extracted topics.\n",
        "    \"\"\"\n",
        "    sen_model = SentenceTransformer(\"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\")\n",
        "    hashtag_topics = []\n",
        "    docs = df.tweet_without_stopwords.to_list()\n",
        "    doc_info_bert = bert.get_document_info(docs)\n",
        "    df[\"text_topic\"] = doc_info_bert.Name\n",
        "    for i in range(len(df)):\n",
        "        hashtag_topics.append(tweet_clustering(df, df.iloc[i].tweet_id, representative_hashtags, sen_model))\n",
        "    df[\"hashtag_topics\"] = hashtag_topics\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_topics_tweet_id(df, tweet_id: int, representative_hashtags, bert):\n",
        "    \"\"\"\n",
        "    Method to create new dataframe with topics extracted from text and hashtags.\n",
        "        :params: \n",
        "                 `df (Dataframe)`: tweet dataset dataframe.\n",
        "                 `representative_hashtags (Dict)`: Hashtag Cluster Matrix with a representative name for each cluster.\n",
        "                 `bert (BERTopic model)`: Document fitted BERTopic model object with defualt embedding.\n",
        "        :return: \n",
        "                 `doc_info_bert (str)`: Text Topic assigned to Tweet.\n",
        "                 `hashtag_topics (list)`: Hashtag Topics assigned to Tweet.\n",
        "    \"\"\"\n",
        "    sen_model = SentenceTransformer(\"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\")\n",
        "    docs = df.tweet_without_stopwords.to_list()\n",
        "    doc_info_bert = bert.get_document_info(docs)\n",
        "    index = df[df[\"tweet_id\"] == tweet_id].index.values[0]\n",
        "    hashtag_topics = tweet_clustering(df, tweet_id, representative_hashtags, sen_model)\n",
        "    return doc_info_bert.iloc[index].Name, hashtag_topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2WQwLsix4orL",
        "outputId": "a6c0c45f-f84f-420a-aee7-4343e708c062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['wisszeitvg', 'ichbinhanna', 'dauerstellen',\n",
              "       'weilwirwissenschaftlieben', 'academictwitter', 'streik',\n",
              "       'ichbinhannaat', 'wissenschaftsfreiheit', 'academia', 'diversity',\n",
              "       'cdu', 'hochschulen', 'phd', 'bmbf', 'ugnovelle',\n",
              "       'waspostdocswollen', 'ichbinreyhan', 'sachsenanhalt',\n",
              "       'gegenwisszeitvg10', 'hochschule', 'hannastreikt',\n",
              "       'zukunftsvertrag', '95vswisszeitvg', 'getorganized',\n",
              "       'hannaimbundestag', 'mlunterfinanziert', 'hitzefrei',\n",
              "       'koalitionsvertrag', 'ausbeutung', 'drittmittel', 'unverzichtbar',\n",
              "       'wiko22', 'pandemie', 'firstgen', 'corona', 'maithinkx',\n",
              "       'ichwerdehannasein', 'bundestagswahl', 'oneofusallofus',\n",
              "       'überstunden', 'innovation', 'stopthecuts', 'fuckademia',\n",
              "       'wisssystemfehler', 'bundestag', 'einkaufszentrum',\n",
              "       'bastaprecariatodistato', 'bafög50', 'rassismus',\n",
              "       'speakupostbelgien', 'frististfrust'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# representative_hashtags = hashtag_groups(read_df)\n",
        "rep_hash = pd.read_parquet(\"rep_hash.parquet\")\n",
        "representative_hashtags = rep_hash.group_name.to_numpy()\n",
        "representative_hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2oiysO_4orL"
      },
      "outputs": [],
      "source": [
        "# new_df = create_topics_df(read_df, representative_hashtags, bert_roberta_50)\n",
        "# new_df.to_parquet(\"tweets_analysis_dataset.parquet\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sX1Mzw9G4orM",
        "outputId": "600b8251-2c61-43c9-d4d2-37a80235d657"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:262: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
            "  context = SSLContext(ssl_version or ssl.PROTOCOL_SSLv23)\n",
            "C:\\Users\\Moinam\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:374: DeprecationWarning: ssl.match_hostname() is deprecated\n",
            "  match_hostname(cert, asserted_hostname)\n",
            "No sentence-transformers model found with name C:\\Users\\Moinam/.cache\\torch\\sentence_transformers\\T-Systems-onsite_cross-en-de-roberta-sentence-transformer. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('0_wisszeitvg_ich_das_die',\n",
              " ['95vswisszeitvg',\n",
              "  'bastaprecariatodistato',\n",
              "  'ausbeutung',\n",
              "  'wissenschaftsfreiheit',\n",
              "  'ichbinhanna',\n",
              "  'wisszeitvg'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_top, hashtag_topic = fetch_topics_tweet_id(read_df, 1405806358335832065, representative_hashtags, bert_roberta_50)\n",
        "text_top, hashtag_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"['Machtmissbrauch', 'Wissenschaft', 'prekär', 'IchbinHanna', 'WissZeitVG', '95vsWissZeitVG']\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "read_df.iloc[read_df[read_df[\"tweet_id\"] == 1405806358335832065].index.values[0]].hashtags"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dbse-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "062b65ef3276ba69a4c844077904084b225211806ab64d3b4ec5aa0504dca686"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
