{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMtbVD244j7B"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5pUUjKU4v0_",
        "outputId": "ff7aca0e-4fa5-4971-b5f0-d0dabc0d8353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "3b6uHA_F4ysT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('final_twitter_data.parquet')\n",
        "df.drop_duplicates(subset=['tweet_id'])"
      ],
      "metadata": {
        "id": "ZKfhVlt-44dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CT7eDKpdR-Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "r7vGdvBPR-Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkYv0BNkWTnm",
        "outputId": "15c1a5da-a018-4818-e1e0-9fba6472c4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/T-Systems-onsite_cross-en-de-roberta-sentence-transformer. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_most_frequent_hashtags(dataset):\n",
        "    dates=list()\n",
        "\n",
        "    for row,record in enumerate(df.hashtags):\n",
        "      if record!='[]'and df['timestamp'][row][0:7]>'2021-05' : \n",
        "        dates.append(df['timestamp'][row][0:7])\n",
        "    date=list()\n",
        "    for dt in dates:\n",
        "      if dt not in date:\n",
        "        date.append(dt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    i=1\n",
        "    j=6\n",
        "    for i in range(1,3):\n",
        "      for j in range(1,13):\n",
        "        if i==1 and j>5 and j<10 or i==2 and j<10:\n",
        "          globals()[f'hash202{i}_0{j}']=list()\n",
        "          globals()[f'hash202{i}_0{j}'].extend([hash for row,hash in enumerate(df['hashtags']) if df['timestamp'][row][0:7]==f'202{i}-0{j}' and hash!='[]'])\n",
        "            \n",
        "        elif i==1 and j>5 and j>=10 or i==2 and j>=10:\n",
        "          globals()[f'hash202{i}_{j}']=list()\n",
        "          globals()[f'hash202{i}_{j}'].extend([hash for row,hash in enumerate(df['hashtags']) if df['timestamp'][row][0:7]==f'202{i}-{j}' and hash!='[]'])\n",
        "          \n",
        "    #preprocessing the hashtags list from specific periods\n",
        "    i=1\n",
        "    j=6\n",
        "    for i in range(1,3):\n",
        "      for j in range(1,13):\n",
        "        if i==1 and j>5 and j<10 or i==2 and j<10:\n",
        "          globals()[f'all_hashtags_202{i}_0{j}']=list()\n",
        "          for s in globals()[f'hash202{i}_0{j}']:\n",
        "              bb=s.split(', ')\n",
        "              a=[re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\",i).group(1) for i in bb]\n",
        "              globals()[f'all_hashtags_202{i}_0{j}'].extend(a)\n",
        "            \n",
        "            \n",
        "        elif i==1 and j>5 and j>=10 or i==2 and j>=10:\n",
        "          globals()[f'all_hashtags_202{i}_{j}']=list()\n",
        "          for s in globals()[f'hash202{i}_{j}']:\n",
        "              bb=s.split(', ')\n",
        "              a=[re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\",i).group(1) for i in bb]\n",
        "              globals()[f'all_hashtags_202{i}_{j}'].extend(a)\n",
        "\n",
        "\n",
        "    def hashtag_set_finder():\n",
        "      hashtag_set=set()\n",
        "      for dt in date:\n",
        "          i,j = dt[3],dt[5:]\n",
        "          lis = globals()[f'all_hashtags_202{i}_{j}']\n",
        "          for i in lis:\n",
        "            hashtag_set.add(i)\n",
        "      return hashtag_set\n",
        "    hashtags_in_DS=hashtag_set_finder()\n",
        "    hashtags_in_DS2 = {i.lower() for i in hashtags_in_DS}\n",
        "    def generate_hashtags(dataset):\n",
        "      listOfAllHashtags=list()\n",
        "      for row, hash in enumerate(dataset.hashtags):\n",
        "        if dataset['timestamp'][row][0:10]>=\"2021-06-01\" and hash!='[]':\n",
        "          bb=hash.split(', ') \n",
        "          \n",
        "          a=[re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\",i).group(1) for i in bb]\n",
        "          listOfAllHashtags.extend(a)\n",
        "      listOfAllHashtagsL=[i.lower() for i in listOfAllHashtags]\n",
        "      return listOfAllHashtagsL\n",
        "    list_of_all_hashtags = generate_hashtags(df)\n",
        "    dic_of_mfh={i:list_of_all_hashtags.count(i)/len(list_of_all_hashtags) for i in hashtags_in_DS2 if list_of_all_hashtags.count(i)>500}\n",
        "      #input_hashtags = [i[0] for i in list_of_mfh]\n",
        "    return dic_of_mfh"
      ],
      "metadata": {
        "id": "uy4tL-RSRV06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTZhZb8fS9Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hashtag_groups():\n",
        "    model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
        "    dic_of_mfh= extract_most_frequent_hashtags(df)\n",
        "    input_hashtags=list(dic_of_mfh.keys())\n",
        "    sentence = input_hashtags\n",
        "    embedding_full = model.encode(sentence)\n",
        "    clusterd_hashtags=list()\n",
        "    for hash in input_hashtags:\n",
        "      embedding = model.encode(hash)\n",
        "      sublist={hash}\n",
        "      for j in range(len(input_hashtags)):\n",
        "        if j<len(input_hashtags)-1:\n",
        "          if util.pytorch_cos_sim(embedding,model.encode(input_hashtags[j+1])) > 0.5:\n",
        "              sublist.add(input_hashtags[j+1])\n",
        "              input_hashtags.remove(input_hashtags[j+1])\n",
        "      clusterd_hashtags.append(sublist)\n",
        "\n",
        "      clusterd_hashtags.sort(reverse=True, key= lambda i: len(i))\n",
        "    print(\"for each cluster a group name has been extracted based on the most frequent hashtag of that group or a Group Representative\\n\")\n",
        "    for i in clusterd_hashtags:\n",
        "      i=list(i)\n",
        "      group_n=i[0]\n",
        "      for j in i:\n",
        "        if dic_of_mfh[j] > dic_of_mfh[group_n]:\n",
        "          group_n=j\n",
        "      print(f\"group name  :  {group_n}\" )\n",
        "      print(f'{i}\\n') "
      ],
      "metadata": {
        "id": "092k2W4L4_XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_groups()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr4ZNVGsRBJX",
        "outputId": "69ba139f-acdb-443a-f22b-b4cb33c7695d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for each cluster a group name has been extracted based on the most frequent hashtag of that group or a Group Representative\n",
            "\n",
            "group name  :  wisszeitvg\n",
            "['woisthanna', 'tvstud', 'wisskomm', 'karliczek', 'wisszeitvg', 'ichwarhanna']\n",
            "\n",
            "group name  :  wissenschaft\n",
            "['wissenschaft', 'bundestag', 'daad', 'postdocs', 'hannainzahlen']\n",
            "\n",
            "group name  :  berlhg\n",
            "['ichbinhannach', 'berlhg', 'bmbf']\n",
            "\n",
            "group name  :  hochschulen\n",
            "['academia', 'hochschule', 'hochschulen']\n",
            "\n",
            "group name  :  dauerstellen\n",
            "['dauerstellen', 'dauerstellenf√ºrdaueraufgaben', 'befristung']\n",
            "\n",
            "group name  :  academictwitter\n",
            "['twittercampus', 'academictwitter']\n",
            "\n",
            "group name  :  hannabeidergew\n",
            "['hannastreikt', 'hannabeidergew']\n",
            "\n",
            "group name  :  27juni\n",
            "['27juni']\n",
            "\n",
            "group name  :  hannaorganisiertsich\n",
            "['hannaorganisiertsich']\n",
            "\n",
            "group name  :  evaluation\n",
            "['evaluation']\n",
            "\n",
            "group name  :  acertaindegreeofflexibility\n",
            "['acertaindegreeofflexibility']\n",
            "\n",
            "group name  :  tenuretrack\n",
            "['tenuretrack']\n",
            "\n",
            "group name  :  corona\n",
            "['corona']\n",
            "\n",
            "group name  :  forschung\n",
            "['forschung']\n",
            "\n",
            "group name  :  wisssystemfehler\n",
            "['wisssystemfehler']\n",
            "\n",
            "group name  :  firstgen\n",
            "['firstgen']\n",
            "\n",
            "group name  :  dasgewinnenwir\n",
            "['dasgewinnenwir']\n",
            "\n",
            "group name  :  hannaimbundestag\n",
            "['hannaimbundestag']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ov4O_34ri8_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}