{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "gHea7JE2xaTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL9qgzYUxl9Q",
        "outputId": "a5d58c4b-52a7-4cf2-df9c-1565f39ea1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "pVvBqnFoxoHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#try the word embedding using the following vectors"
      ],
      "metadata": {
        "id": "dNrFBfyyqoCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# @thesis{mueller2015,\n",
        "  author = {{Müller}, Andreas},\n",
        "  title  = \"{Analyse von Wort-Vektoren deutscher Textkorpora}\",\n",
        "  school = {Technische Universität Berlin},\n",
        "  year   = 2015,\n",
        "  month  = jun,\n",
        "  type   = {Bachelor's Thesis},\n",
        "  url    = {https://devmount.github.io/GermanWordEmbeddings}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "baIw2RCY3GnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import pickle\n",
        "\n",
        "#parser = argparse.ArgumentParser(description='Script for visualizing word vector models')\n",
        "#parser.add_argument('model', type=str, help='source file with trained model')\n",
        "#args = parser.parse_args()\n",
        "#model = pickle.load(open(\"german.model\",'r',encoding='latin-1'))\n",
        "with open(\"german.model\",'rb') as file: #,encoding='latin-1'\n",
        "  model=file.read()\n",
        "# configuration\n",
        "currency = [\n",
        "    'Schweiz', 'Franken', 'Deutschland', 'Euro', 'Grossbritannien', 'britische_Pfund',\n",
        "    'Japan', 'Yen', 'Russland', 'Rubel', 'USA', 'US-Dollar', 'Kroatien', 'Kuna'\n",
        "]\n",
        "capital = [\n",
        "    'Athen', 'Griechenland', 'Berlin', 'Deutschland', 'Ankara', 'Tuerkei', 'Bern', 'Schweiz', 'Hanoi', 'Vietnam',\n",
        "    'Lissabon', 'Portugal', 'Moskau', 'Russland', 'Stockholm', 'Schweden', 'Tokio', 'Japan', 'Washington', 'USA'\n",
        "]\n",
        "language = [\n",
        "    'Deutschland', 'Deutsch', 'USA', 'Englisch', 'Frankreich', 'Franzoesisch', 'Griechenland', 'Griechisch',\n",
        "    'Norwegen', 'Norwegisch', 'Schweden', 'Schwedisch', 'Polen', 'Polnisch', 'Ungarn', 'Ungarisch'\n",
        "]\n",
        "\n",
        "# matches = model.most_similar(positive=[\"Frau\"], negative=[], topn=30)\n",
        "# words = [match[0] for match in matches]\n",
        "\n",
        "\n",
        "def draw_words(model, words, pca=False, alternate=True, arrows=True, x1=3, x2=3, y1=3, y2=3, title=''):\n",
        "    \"\"\"\n",
        "    Reduces dimensionality of vectors of given words either with PCA or with t-SNE and draws the words into a diagram.\n",
        "\n",
        "    :param model: to visualize vectors from\n",
        "    :param words: list of word strings to visualize\n",
        "    :param pca: use PCA (True) or t-SNE (False) to reduce dimensionality\n",
        "    :param alternate: use different color and label align for every second word\n",
        "    :param arrows: use arrows to connect related words (items that are next to each other in list)\n",
        "    :param x1: x axis range (from)\n",
        "    :param x2: x axis range (to)\n",
        "    :param y1: y axis range (from)\n",
        "    :param y2: y axis range (to)\n",
        "    :param title: for diagram\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # get vectors for given words from model\n",
        "    vectors = [model[word] for word in words]\n",
        "\n",
        "    if pca:\n",
        "        pca = PCA(n_components=2, whiten=True)\n",
        "        vectors2d = pca.fit(vectors).transform(vectors)\n",
        "    else:\n",
        "        tsne = TSNE(n_components=2, random_state=0)\n",
        "        vectors2d = tsne.fit_transform(vectors)\n",
        "\n",
        "    # draw image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    if pca:\n",
        "        plt.axis([x1, x2, y1, y2])\n",
        "\n",
        "    first = True  # color alternation to divide given groups\n",
        "    for point, word in zip(vectors2d, words):\n",
        "        # plot points\n",
        "        plt.scatter(point[0], point[1], c='r' if first else 'g')\n",
        "        # plot word annotations\n",
        "        plt.annotate(\n",
        "            word, \n",
        "            xy=(point[0], point[1]),\n",
        "            xytext=(-7, -6) if first else (7, -6),\n",
        "            textcoords='offset points',\n",
        "            ha='right' if first else 'left',\n",
        "            va='bottom',\n",
        "            size=\"x-large\"\n",
        "        )\n",
        "        first = not first if alternate else first\n",
        "\n",
        "    # draw arrows\n",
        "    if arrows:\n",
        "        for i in range(0, len(words)-1, 2):\n",
        "            a = vectors2d[i][0] + 0.04\n",
        "            b = vectors2d[i][1]\n",
        "            c = vectors2d[i+1][0] - 0.04\n",
        "            d = vectors2d[i+1][1]\n",
        "            plt.arrow(\n",
        "                a, b, c-a, d-b,\n",
        "                shape='full',\n",
        "                lw=0.1,\n",
        "                edgecolor='#bbbbbb',\n",
        "                facecolor='#bbbbbb',\n",
        "                length_includes_head=True,\n",
        "                head_width=0.08,\n",
        "                width=0.01\n",
        "            )\n",
        "\n",
        "    # draw diagram title\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# get trained model\n",
        "\n",
        "trained_model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
        "\n",
        "# draw pca plots\n",
        "draw_words(trained_model, currency, True, True, True, -3, 3, -2, 2, r'$PCA\\ Visualisierung:\\ W\\ddot{a}hrung$')\n",
        "draw_words(trained_model, capital, True, True, True, -3, 3, -2, 2.2, r'$PCA\\ Visualisierung:\\ Hauptstadt$')\n",
        "draw_words(trained_model, language, True, True, True, -3, 3, -2, 1.7, r'$PCA\\ Visualisierung:\\ Sprache$')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-79O1jG3Lxg",
        "outputId": "f94d42f7-c5ec-45b8-bfa5-a0770a97392a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZGQ1P21q05g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#set of all hashtags from the dataset"
      ],
      "metadata": {
        "id": "XMAMq9OoxV2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('final_twitter_data.parquet')\n",
        "df.drop_duplicates(subset=['tweet_id'])"
      ],
      "metadata": {
        "id": "I-GuYr3CyaLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates=list()\n",
        "\n",
        "for row,record in enumerate(df.hashtags):\n",
        "  if record!='[]'and df['timestamp'][row][0:7]>'2021-05' : \n",
        "    dates.append(df['timestamp'][row][0:7])\n",
        "date=list()\n",
        "for dt in dates:\n",
        "  if dt not in date:\n",
        "    date.append(dt)"
      ],
      "metadata": {
        "id": "SuRbuHVQYUel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-rf7L-_R0q-"
      },
      "outputs": [],
      "source": [
        "i=1\n",
        "j=6\n",
        "for i in range(1,3):\n",
        "  for j in range(1,13):\n",
        "    if i==1 and j>5 and j<10 or i==2 and j<10:\n",
        "      globals()[f'hash202{i}_0{j}']=list()\n",
        "      globals()[f'hash202{i}_0{j}'].extend([hash for row,hash in enumerate(df['hashtags']) if df['timestamp'][row][0:7]==f'202{i}-0{j}' and hash!='[]'])\n",
        "        \n",
        "    elif i==1 and j>5 and j>=10 or i==2 and j>=10:\n",
        "      globals()[f'hash202{i}_{j}']=list()\n",
        "      globals()[f'hash202{i}_{j}'].extend([hash for row,hash in enumerate(df['hashtags']) if df['timestamp'][row][0:7]==f'202{i}-{j}' and hash!='[]'])\n",
        "      \n",
        "#preprocessing the hashtags list from specific periods\n",
        "i=1\n",
        "j=6\n",
        "for i in range(1,3):\n",
        "  for j in range(1,13):\n",
        "    if i==1 and j>5 and j<10 or i==2 and j<10:\n",
        "      globals()[f'all_hashtags_202{i}_0{j}']=list()\n",
        "      for s in globals()[f'hash202{i}_0{j}']:\n",
        "          bb=s.split(', ')\n",
        "          a=[re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\",i).group(1) for i in bb]\n",
        "          globals()[f'all_hashtags_202{i}_0{j}'].extend(a)\n",
        "        \n",
        "        \n",
        "    elif i==1 and j>5 and j>=10 or i==2 and j>=10:\n",
        "      globals()[f'all_hashtags_202{i}_{j}']=list()\n",
        "      for s in globals()[f'hash202{i}_{j}']:\n",
        "          bb=s.split(', ')\n",
        "          a=[re.search(r\"[\\[\\']*(\\w*)[\\]\\']*\",i).group(1) for i in bb]\n",
        "          globals()[f'all_hashtags_202{i}_{j}'].extend(a)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4UQ9Xpfux4kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hashtag_set_finder():\n",
        "  hashtag_set=set()\n",
        "  for dt in date:\n",
        "      i,j = dt[3],dt[5:]\n",
        "      lis = globals()[f'all_hashtags_202{i}_{j}']\n",
        "      for i in lis:\n",
        "        hashtag_set.add(i)\n",
        "  return hashtag_set"
      ],
      "metadata": {
        "id": "X1q0jMtpx5Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags_in_DS=hashtag_set_finder()"
      ],
      "metadata": {
        "id": "u3UqDnk-x_hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags_in_DS"
      ],
      "metadata": {
        "id": "3WnXr2fNzHLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMUAMekczMzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nrv8cidGyemJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}